"""
Apache Iceberg Demo 7: Branching, Tagging & WAP
===============================================

This demo covers:
- Apache Iceberg: CREATE BRANCH
- Apache Iceberg: CREATE TAG
- Apache Iceberg: Writing to Branches
- Apache Iceberg: Branch Retention Policies
- Apache Iceberg: WAP (Write-Audit-Publish)

Run with:
docker exec -it spark-master /opt/bitnami/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  /opt/bitnami/spark/jobs/spark/mod-4/demo-13.py
"""

import base64
import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col


def spark_session():
    """Create Spark Session with Apache Iceberg and MinIO support"""

    encoded_access_key = "bWluaW9sYWtl"
    encoded_secret_key = "TGFrRTE0MjUzNkBA"
    access_key = base64.b64decode(encoded_access_key).decode("utf-8")
    secret_key = base64.b64decode(encoded_secret_key).decode("utf-8")

    spark = SparkSession.builder \
        .appName("IcebergDemo7-BranchingTagging") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config("spark.sql.catalog.hadoop_catalog", "org.apache.iceberg.spark.SparkCatalog") \
        .config("spark.sql.catalog.hadoop_catalog.type", "hadoop") \
        .config("spark.sql.catalog.hadoop_catalog.warehouse", "s3a://owshq-catalog/warehouse") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://24.144.65.249:80") \
        .config("spark.hadoop.fs.s3a.access.key", access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", secret_key) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    print("‚úÖ Apache Iceberg Spark Session Created Successfully!")
    print(f"üöÄ Spark Version: {spark.version}")

    return spark


def setup_namespace(spark):
    """Setup namespace for demo"""

    print("\n=== Setting Up Demo Namespace ===")

    # TODO create namespace
    print("üìÅ creating namespace...")
    spark.sql("CREATE NAMESPACE IF NOT EXISTS hadoop_catalog.ubereats_demo7")

    # TODO set catalog context
    spark.catalog.setCurrentCatalog("hadoop_catalog")
    spark.catalog.setCurrentDatabase("ubereats_demo7")

    print("‚úÖ namespace ready!")


def create_branch(spark):
    """Demonstrate CREATE BRANCH"""

    print("\n=== Apache Iceberg: CREATE BRANCH ===")

    table_fq = "hadoop_catalog.ubereats_demo7.orders"

    # TODO create base table
    print("üèóÔ∏è creating base table...")
    spark.sql(f"""
              CREATE TABLE IF NOT EXISTS {table_fq}
              (
                  order_id STRING,
                  user_id INT,
                  amount DOUBLE,
                  status STRING
              ) USING iceberg
              """)

    # TODO insert initial data
    print("üíæ inserting initial data...")
    spark.sql(f"""
              INSERT INTO {table_fq} VALUES
              ('ORD-001', 1001, 25.50, 'completed'),
              ('ORD-002', 1002, 18.75, 'pending')
              """)

    # TODO create branches
    print("üåø creating development branch...")
    spark.sql(f"ALTER TABLE {table_fq} CREATE BRANCH development")

    print("üåø creating feature branch...")
    spark.sql(f"ALTER TABLE {table_fq} CREATE BRANCH feature")

    # TODO show branches
    print("üîç showing branches...")
    spark.sql(f"SELECT name, type FROM {table_fq}.refs").show()

    print("‚úÖ branches created!")


def create_tag(spark):
    """Demonstrate CREATE TAG"""

    print("\n=== Apache Iceberg: CREATE TAG ===")

    table_fq = "hadoop_catalog.ubereats_demo7.orders"

    # TODO create tag
    print("üè∑Ô∏è creating release tag...")
    spark.sql(f"ALTER TABLE {table_fq} CREATE TAG v1_0")

    # TODO add more data
    print("üíæ adding more data...")
    spark.sql(f"INSERT INTO {table_fq} VALUES ('ORD-003', 1003, 32.00, 'processing')")

    # TODO create another tag
    print("üè∑Ô∏è creating v1.1 tag...")
    spark.sql(f"ALTER TABLE {table_fq} CREATE TAG v1_1")

    # TODO show all refs
    print("üîç showing all refs...")
    spark.sql(f"SELECT name, type FROM {table_fq}.refs ORDER BY type, name").show()

    print("‚úÖ tags created!")


def writing_to_branches(spark):
    """Demonstrate Writing to Branches"""

    print("\n=== Apache Iceberg: Writing to Branches ===")

    table_fq = "hadoop_catalog.ubereats_demo7.orders"

    # TODO write to development branch
    print("üíæ writing to development branch...")
    spark.sql(f"""
              INSERT INTO {table_fq}.branch_development VALUES
              ('ORD-DEV-001', 2001, 35.75, 'testing')
              """)

    # TODO write to feature branch
    print("üíæ writing to feature branch...")
    spark.sql(f"""
              INSERT INTO {table_fq}.branch_feature VALUES
              ('ORD-FEAT-001', 3001, 15.50, 'discounted')
              """)

    # TODO compare counts
    print("üîç comparing branch data...")
    print("   üìä Main branch:")
    spark.sql(f"SELECT COUNT(*) as count FROM {table_fq}").show()

    print("   üìä Development branch:")
    spark.sql(f"SELECT COUNT(*) as count FROM {table_fq}.branch_development").show()

    print("   üìä Feature branch:")
    spark.sql(f"SELECT COUNT(*) as count FROM {table_fq}.branch_feature").show()

    print("‚úÖ branch writing demonstrated!")


def branch_retention_policies(spark):
    """
    Demonstrate branch retention policies in Apache Iceberg (Spark 3.5, Iceberg 1.9.1).
    - Drops the branch if it exists, then creates it with a retention policy.
    - Lists branch retention policies using supported columns.
    """

    print("\n=== Apache Iceberg: Branch Retention Policies ===")

    table_fq = "hadoop_catalog.ubereats_demo7.orders"
    branch_name = "temp"
    retention_days = 7

    # Drop the branch if it exists
    print(f"üßπ Dropping branch '{branch_name}' if it exists...")
    try:
        spark.sql(f"ALTER TABLE {table_fq} DROP BRANCH {branch_name}")
        print(f"‚úÖ Branch '{branch_name}' dropped.")
    except Exception as ex:
        if "not found" in str(ex).lower() or "does not exist" in str(ex).lower():
            print(f"‚ÑπÔ∏è Branch '{branch_name}' does not exist, skipping drop.")
        else:
            print(f"‚ö†Ô∏è Unexpected error while dropping branch: {ex}")

    # Create the branch with retention
    print(f"üåø Creating branch '{branch_name}' with {retention_days} day retention...")
    spark.sql(f"ALTER TABLE {table_fq} CREATE BRANCH {branch_name} RETAIN {retention_days} DAYS")

    # Show all branch retention policies using supported columns
    print("üîç Showing branch retention policies...")
    spark.sql(f"""
        SELECT name, type, max_reference_age_in_ms, min_snapshots_to_keep, max_snapshot_age_in_ms
        FROM {table_fq}.refs
        WHERE type = 'BRANCH'
    """).show(truncate=False)

    print("‚úÖ Branch retention policies demonstration complete!")



def write_audit_publish(spark):
    """Demonstrate WAP (Write-Audit-Publish)"""

    print("\n=== Apache Iceberg: WAP (Write-Audit-Publish) ===")

    table_fq = "hadoop_catalog.ubereats_demo7.orders_wap"

    # TODO create WAP table
    print("üèóÔ∏è creating WAP table...")
    spark.sql(f"""
              CREATE TABLE IF NOT EXISTS {table_fq}
              (
                  order_id STRING,
                  amount DOUBLE,
                  status STRING
              ) USING iceberg
              """)

    # TODO insert production data
    print("üíæ inserting production data...")
    spark.sql(f"""
              INSERT INTO {table_fq} VALUES
              ('ORD-PROD-001', 25.50, 'completed'),
              ('ORD-PROD-002', 18.75, 'pending')
              """)

    # TODO WRITE phase: create staging branch
    print("‚úçÔ∏è WAP WRITE: creating staging branch...")
    spark.sql(f"ALTER TABLE {table_fq} CREATE BRANCH staging")

    print("‚úçÔ∏è WAP WRITE: writing to staging...")
    spark.sql(f"""
              INSERT INTO {table_fq}.branch_staging VALUES
              ('ORD-STAGE-001', 45.90, 'pending'),
              ('ORD-STAGE-002', -5.00, 'invalid')
              """)

    # TODO AUDIT phase: validate data
    print("üîç WAP AUDIT: validating data...")

    negative_count = spark.sql(f"""
                              SELECT COUNT(*) as count 
                              FROM {table_fq}.branch_staging 
                              WHERE amount < 0
                              """).collect()[0]['count']

    print(f"   ‚ùå Negative amounts found: {negative_count}")
    is_valid = negative_count == 0
    print(f"   ‚úÖ Validation: {'PASSED' if is_valid else 'FAILED'}")

    # TODO PUBLISH phase: conditional merge
    if is_valid:
        print("üì§ WAP PUBLISH: publishing to production...")
        spark.sql(f"""
                  INSERT INTO {table_fq}
                  SELECT * FROM {table_fq}.branch_staging WHERE amount > 0
                  """)
        print("   ‚úÖ Data published successfully!")
    else:
        print("   ‚ùå Data NOT published - validation failed")

    # TODO show final state
    print("üîç final production data...")
    spark.sql(f"SELECT * FROM {table_fq} ORDER BY order_id").show()

    # TODO cleanup
    print("üßπ cleaning up staging branch...")
    spark.sql(f"ALTER TABLE {table_fq} DROP BRANCH staging")

    print("‚úÖ WAP demonstrated!")


def cleanup_resources(spark):
    """Clean up demo resources"""

    print("\n=== Cleanup ===")

    try:
        # TODO drop tables
        tables = [
            'hadoop_catalog.ubereats_demo7.orders',
            'hadoop_catalog.ubereats_demo7.orders_wap'
        ]

        for table in tables:
            spark.sql(f"DROP TABLE IF EXISTS {table}")

        # TODO drop namespace
        spark.sql("DROP NAMESPACE IF EXISTS hadoop_catalog.ubereats_demo7 CASCADE")

        print("‚úÖ demo resources cleaned up successfully!")

    except Exception as e:
        print(f"‚ö†Ô∏è cleanup warning: {e}")


def main():
    """Main demo execution"""

    print("üöÄ Starting Apache Iceberg Demo 7: Branching, Tagging & WAP")
    print("=" * 70)

    # TODO create Spark session
    spark = spark_session()

    try:
        # TODO run demo sections
        # setup_namespace(spark)
        # create_branch(spark)
        # create_tag(spark)
        # writing_to_branches(spark)
        branch_retention_policies(spark)
        write_audit_publish(spark)

        print("\n" + "=" * 70)
        print("üéâ Demo 7 completed successfully!")
        print("üìö Key concepts covered:")
        print("   ‚úì Creating branches for development")
        print("   ‚úì Creating tags for versioning")
        print("   ‚úì Writing to specific branches")
        print("   ‚úì Branch retention policies")
        print("   ‚úì Write-Audit-Publish pattern")

        print("\nüîó What's Next:")
        print("   ‚Üí Demo 8: Performance & Maintenance")

    except Exception as e:
        print(f"‚ùå Demo failed with error: {e}")
        import traceback
        traceback.print_exc()

    finally:
        # TODO cleanup
        # cleanup_resources(spark)
        spark.stop()
        print("üîí Spark session stopped")


if __name__ == "__main__":
    main()
