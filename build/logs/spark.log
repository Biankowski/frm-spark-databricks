2025-04-02T15:01:49,646 [main] INFO  org.apache.spark.deploy.master.Master [] - Started daemon with process name: 62@07562851e9bf
2025-04-02T15:01:49,650 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T15:01:49,651 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T15:01:49,652 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T15:01:49,677 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@68f3e7a714d1
2025-04-02T15:01:49,686 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T15:01:49,689 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T15:01:49,690 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T15:01:49,791 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@bc42895a3022
2025-04-02T15:01:49,797 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T15:01:49,798 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T15:01:49,798 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T15:01:49,806 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@b568520b94cf
2025-04-02T15:01:49,810 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T15:01:49,811 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T15:01:49,811 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T15:01:49,827 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T15:01:49,828 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T15:01:49,828 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T15:01:49,829 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T15:01:49,829 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T15:01:49,834 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T15:01:49,836 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T15:01:49,836 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T15:01:49,837 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T15:01:49,840 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T15:01:49,992 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T15:01:49,996 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T15:01:49,994 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T15:01:50,003 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T15:01:50,009 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T15:01:50,018 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T15:01:50,018 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T15:01:50,023 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T15:01:50,026 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T15:01:50,034 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T15:01:50,039 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T15:01:50,067 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T15:01:50,282 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T15:01:50,306 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T15:01:50,448 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkMaster' on port 7077.
2025-04-02T15:01:50,450 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 42425.
2025-04-02T15:01:50,452 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-02T15:01:50,468 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Starting Spark master at spark://07562851e9bf:7077
2025-04-02T15:01:50,474 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Running Spark version 3.5.5
2025-04-02T15:01:50,493 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1805ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T15:01:50,513 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 33947.
2025-04-02T15:01:50,516 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-02T15:01:50,532 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8080 for MasterUI
2025-04-02T15:01:50,537 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 39551.
2025-04-02T15:01:50,540 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-02T15:01:50,542 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T15:01:50,572 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.3:42425 with 2 cores, 3.0 GiB RAM
2025-04-02T15:01:50,573 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @1884ms
2025-04-02T15:01:50,581 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-02T15:01:50,582 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-02T15:01:50,609 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T15:01:50,610 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-02T15:01:50,611 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T15:01:50,629 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@b12b344{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
2025-04-02T15:01:50,630 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'MasterUI' on port 8080.
2025-04-02T15:01:50,642 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1872ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T15:01:50,652 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.4:33947 with 2 cores, 3.0 GiB RAM
2025-04-02T15:01:50,660 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-02T15:01:50,660 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-02T15:01:50,663 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3a8f3f7d{/app,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,666 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@59ddca39{/app/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,668 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@f0c030b{/,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,669 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40c0c815{/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,672 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.5:39551 with 2 cores, 3.0 GiB RAM
2025-04-02T15:01:50,673 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2b57827f{/static,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,673 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T15:01:50,674 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-02T15:01:50,674 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T15:01:50,674 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28ac9723{/app/kill,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,676 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-02T15:01:50,676 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@70c7f9ab{/driver/kill,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,677 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-02T15:01:50,678 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1cc9caaa{/workers/kill,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,680 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-02T15:01:50,681 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.ui.MasterWebUI [] - Bound MasterWebUI to 0.0.0.0, and started at http://07562851e9bf:8080
2025-04-02T15:01:50,690 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T15:01:50,698 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T15:01:50,698 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-02T15:01:50,699 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T15:01:50,710 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1914ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T15:01:50,710 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @1941ms
2025-04-02T15:01:50,733 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1938ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T15:01:50,740 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-02T15:01:50,741 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@f224e17{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-02T15:01:50,742 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-02T15:01:50,764 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T15:01:50,779 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-02T15:01:50,786 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @1991ms
2025-04-02T15:01:50,785 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@316d2907{/logPage,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,791 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64ca7b3c{/logPage/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,793 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T15:01:50,793 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3644aba7{/metrics/master/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,794 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1b03e088{/,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,795 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@44bd3784{/metrics/applications/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,796 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@217e4d76{/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,803 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3261b7be{/static,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,806 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @2011ms
2025-04-02T15:01:50,808 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@e2d614f{/log,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,808 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - I have been elected leader! New state: ALIVE
2025-04-02T15:01:50,811 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://68f3e7a714d1:8081
2025-04-02T15:01:50,815 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-02T15:01:50,821 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2128c8f0{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-02T15:01:50,823 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-02T15:01:50,834 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@4a6457b6{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-02T15:01:50,835 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-02T15:01:50,840 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1e371437{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,848 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48cebe6d{/logPage,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,850 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ec419dc{/logPage/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,853 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1510a7a4{/,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,856 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@77936a8a{/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,859 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@dd56c43{/logPage,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,863 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e7739fd{/static,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,866 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@524756b{/log,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,867 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://bc42895a3022:8081
2025-04-02T15:01:50,869 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@63f0fc5{/logPage/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,873 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36ab0bcd{/,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,870 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-02T15:01:50,875 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6acdf2d1{/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,876 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 33 ms (0 ms spent in bootstraps)
2025-04-02T15:01:50,880 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6f4905e1{/static,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,881 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d471fe3{/log,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,886 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://b568520b94cf:8081
2025-04-02T15:01:50,890 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-02T15:01:50,900 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@41e3cc07{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,908 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5ae015a7{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T15:01:50,940 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 29 ms (0 ms spent in bootstraps)
2025-04-02T15:01:50,956 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 27 ms (0 ms spent in bootstraps)
2025-04-02T15:01:51,006 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.3:42425 with 2 cores, 3.0 GiB RAM
2025-04-02T15:01:51,013 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.4:33947 with 2 cores, 3.0 GiB RAM
2025-04-02T15:01:51,014 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.5:39551 with 2 cores, 3.0 GiB RAM
2025-04-02T15:01:51,018 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://07562851e9bf:7077
2025-04-02T15:01:51,022 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://07562851e9bf:7077
2025-04-02T15:01:51,023 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://07562851e9bf:7077
2025-04-02T15:03:55,689 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.3:52472 got disassociated, removing it.
2025-04-02T15:03:55,693 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.5:34318 got disassociated, removing it.
2025-04-02T15:03:55,693 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.4:42794 got disassociated, removing it.
2025-04-02T15:03:55,694 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.4:33947 got disassociated, removing it.
2025-04-02T15:03:55,695 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250402150150-172.18.0.4-33947 on 172.18.0.4:33947
2025-04-02T15:03:55,697 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250402150150-172.18.0.4-33947
2025-04-02T15:03:55,699 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.5:39551 got disassociated, removing it.
2025-04-02T15:03:55,700 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250402150150-172.18.0.5-39551 on 172.18.0.5:39551
2025-04-02T15:03:55,700 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250402150150-172.18.0.5-39551
2025-04-02T15:03:55,700 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.3:42425 got disassociated, removing it.
2025-04-02T15:03:55,701 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Removing worker worker-20250402150150-172.18.0.3-42425 on 172.18.0.3:42425
2025-04-02T15:03:55,701 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Telling app of lost worker: worker-20250402150150-172.18.0.3-42425
2025-04-02T17:48:22,115 [main] INFO  org.apache.spark.deploy.master.Master [] - Started daemon with process name: 62@eff501c685cd
2025-04-02T17:48:22,116 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@d332fd2e5338
2025-04-02T17:48:22,126 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T17:48:22,127 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T17:48:22,127 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T17:48:22,128 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T17:48:22,127 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T17:48:22,132 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T17:48:22,203 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@046119187e0d
2025-04-02T17:48:22,209 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T17:48:22,210 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T17:48:22,210 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T17:48:22,266 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Started daemon with process name: 60@44b70a14a7a1
2025-04-02T17:48:22,273 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T17:48:22,274 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T17:48:22,275 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T17:48:22,301 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T17:48:22,301 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T17:48:22,302 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T17:48:22,303 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T17:48:22,304 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T17:48:22,328 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T17:48:22,329 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T17:48:22,334 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T17:48:22,335 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T17:48:22,335 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T17:48:22,348 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T17:48:22,350 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T17:48:22,353 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T17:48:22,354 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T17:48:22,355 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T17:48:22,405 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T17:48:22,407 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T17:48:22,411 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T17:48:22,416 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T17:48:22,417 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T17:48:22,451 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T17:48:22,465 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T17:48:22,485 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T17:48:22,555 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T17:48:22,766 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 37899.
2025-04-02T17:48:22,767 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-02T17:48:22,790 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 45421.
2025-04-02T17:48:22,792 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-02T17:48:22,806 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkWorker' on port 35613.
2025-04-02T17:48:22,808 [main] INFO  org.apache.spark.deploy.worker.Worker [] - Worker decommissioning not enabled.
2025-04-02T17:48:22,816 [main] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkMaster' on port 7077.
2025-04-02T17:48:22,831 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Starting Spark master at spark://eff501c685cd:7077
2025-04-02T17:48:22,836 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Running Spark version 3.5.5
2025-04-02T17:48:22,868 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1790ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T17:48:22,899 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.4:37899 with 2 cores, 3.0 GiB RAM
2025-04-02T17:48:22,907 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-02T17:48:22,909 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-02T17:48:22,925 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8080 for MasterUI
2025-04-02T17:48:22,926 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.5:45421 with 2 cores, 3.0 GiB RAM
2025-04-02T17:48:22,929 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T17:48:22,931 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-02T17:48:22,933 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-02T17:48:22,932 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-02T17:48:22,934 [dispatcher-event-loop-1] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T17:48:22,934 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T17:48:22,945 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T17:48:22,949 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-02T17:48:22,951 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T17:48:22,954 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Starting Spark worker 172.18.0.3:35613 with 2 cores, 3.0 GiB RAM
2025-04-02T17:48:22,954 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @1877ms
2025-04-02T17:48:22,957 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Running Spark version 3.5.5
2025-04-02T17:48:22,958 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Spark home: /opt/bitnami/spark
2025-04-02T17:48:22,971 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1806ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T17:48:22,974 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T17:48:22,975 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.worker.
2025-04-02T17:48:22,976 [dispatcher-event-loop-0] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T17:48:22,987 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1808ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T17:48:22,987 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@58e53864{HTTP/1.1, (http/1.1)}{0.0.0.0:8080}
2025-04-02T17:48:22,987 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'MasterUI' on port 8080.
2025-04-02T17:48:22,998 [dispatcher-event-loop-1] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-02T17:48:23,004 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1779ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T17:48:23,009 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T17:48:23,010 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@17704a1f{/app,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,012 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d2c8b5f{/app/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,013 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7062fc1e{/,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,015 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4904b535{/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,016 [dispatcher-event-loop-0] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-02T17:48:23,024 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@68c331e6{/static,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,025 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5bea2460{/app/kill,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,026 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1533fdb3{/driver/kill,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,027 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T17:48:23,027 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ec81a62{/workers/kill,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,030 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.ui.MasterWebUI [] - Bound MasterWebUI to 0.0.0.0, and started at http://eff501c685cd:8080
2025-04-02T17:48:23,033 [dispatcher-event-loop-0] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:8081 for WorkerUI
2025-04-02T17:48:23,037 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.Server [] - Started @1872ms
2025-04-02T17:48:23,039 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T17:48:23,055 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.Server [] - Started @1877ms
2025-04-02T17:48:23,067 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.Server [] - Started @1843ms
2025-04-02T17:48:23,069 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@413cc8b0{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-02T17:48:23,070 [dispatcher-event-loop-1] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-02T17:48:23,084 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2f2766c{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-02T17:48:23,085 [dispatcher-event-loop-0] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-02T17:48:23,086 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@58e8b7ba{HTTP/1.1, (http/1.1)}{0.0.0.0:8081}
2025-04-02T17:48:23,086 [dispatcher-event-loop-0] INFO  org.apache.spark.util.Utils [] - Successfully started service 'WorkerUI' on port 8081.
2025-04-02T17:48:23,088 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5de1a79c{/logPage,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,090 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@627db294{/logPage/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,092 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@77b7837e{/,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,095 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@116a871d{/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,096 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37a45449{/logPage,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,099 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1fbdfe1d{/logPage/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,100 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@79458b7a{/,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,101 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29095749{/static,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,101 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@637e2a1c{/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,102 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@bb787f7{/log,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,104 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://046119187e0d:8081
2025-04-02T17:48:23,105 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48a7d6d2{/logPage,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,107 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@79531e4d{/static,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,108 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@474e5743{/logPage/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,106 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-02T17:48:23,108 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@18c9c870{/log,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,109 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b5ca91a{/,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,110 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://d332fd2e5338:8081
2025-04-02T17:48:23,111 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d33361b{/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,112 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-02T17:48:23,115 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@566c222{/static,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,117 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@63363a78{/log,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,118 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.ui.WorkerWebUI [] - Bound WorkerWebUI to 0.0.0.0, and started at http://44b70a14a7a1:8081
2025-04-02T17:48:23,122 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@a7efbac{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,121 [worker-register-master-threadpool-0] INFO  org.apache.spark.deploy.worker.Worker [] - Connecting to master spark-master:7077...
2025-04-02T17:48:23,126 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@513bf9e5{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,128 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@21139305{/metrics/master/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,129 [dispatcher-event-loop-1] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@55781b4c{/metrics/applications/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,135 [dispatcher-event-loop-0] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@626f28b5{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T17:48:23,136 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - I have been elected leader! New state: ALIVE
2025-04-02T17:48:23,150 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 24 ms (0 ms spent in bootstraps)
2025-04-02T17:48:23,152 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 21 ms (0 ms spent in bootstraps)
2025-04-02T17:48:23,178 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 21 ms (0 ms spent in bootstraps)
2025-04-02T17:48:23,245 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.5:45421 with 2 cores, 3.0 GiB RAM
2025-04-02T17:48:23,253 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.3:35613 with 2 cores, 3.0 GiB RAM
2025-04-02T17:48:23,255 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://eff501c685cd:7077
2025-04-02T17:48:23,256 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Registering worker 172.18.0.4:37899 with 2 cores, 3.0 GiB RAM
2025-04-02T17:48:23,266 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://eff501c685cd:7077
2025-04-02T17:48:23,269 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Successfully registered with master spark://eff501c685cd:7077
2025-04-02T18:01:06,011 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:01:06,014 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:01:06,014 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:01:06,026 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:01:06,026 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:01:06,027 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:01:06,027 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: pr-3-app.py
2025-04-02T18:01:06,035 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:01:06,039 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:01:06,039 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:01:06,064 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:01:06,065 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:01:06,065 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:06,066 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:06,066 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:01:06,098 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:01:06,205 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 43919.
2025-04-02T18:01:06,221 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:01:06,237 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:01:06,245 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:01:06,246 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:01:06,248 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:01:06,258 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-9fa83cc9-928a-4ab7-9e71-d5b389f9e981
2025-04-02T18:01:06,264 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:01:06,271 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:01:06,292 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1358ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:01:06,331 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:01:06,337 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:01:06,346 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1413ms
2025-04-02T18:01:06,360 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@a41bc5b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:01:06,361 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:01:06,371 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@433931f8{/,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,421 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:01:06,445 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 12 ms (0 ms spent in bootstraps)
2025-04-02T18:01:06,504 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app pr-3-app.py
2025-04-02T18:01:06,514 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app pr-3-app.py with ID app-20250402180106-0000
2025-04-02T18:01:06,517 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180106-0000 with rpId: 0
2025-04-02T18:01:06,519 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180106-0000
2025-04-02T18:01:06,525 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42721.
2025-04-02T18:01:06,526 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:42721
2025-04-02T18:01:06,528 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:01:06,533 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 42721, None)
2025-04-02T18:01:06,536 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:42721 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 42721, None)
2025-04-02T18:01:06,537 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 42721, None)
2025-04-02T18:01:06,540 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180106-0000/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:01:06,542 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 42721, None)
2025-04-02T18:01:06,549 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180106-0000/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:01:06,550 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180106-0000/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:01:06,552 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180106-0000/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:01:06,554 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180106-0000/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:01:06,555 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180106-0000/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:01:06,556 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180106-0000/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:01:06,557 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180106-0000/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:01:06,558 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180106-0000/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:01:06,600 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180106-0000/1 for pr-3-app.py
2025-04-02T18:01:06,600 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180106-0000/0 for pr-3-app.py
2025-04-02T18:01:06,609 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180106-0000/2 for pr-3-app.py
2025-04-02T18:01:06,628 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:01:06,629 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:01:06,629 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:06,629 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:06,629 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:01:06,630 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:01:06,631 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:01:06,631 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:06,631 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:06,632 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:01:06,643 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43919" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:43919" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180106-0000" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:01:06,645 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43919" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:43919" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180106-0000" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:01:06,646 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:01:06,647 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:01:06,648 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:06,648 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:06,648 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:01:06,670 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43919" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:43919" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180106-0000" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:01:06,683 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180106-0000 with rpId: 0
2025-04-02T18:01:06,686 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180106-0000 with rpId: 0
2025-04-02T18:01:06,695 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180106-0000 with rpId: 0
2025-04-02T18:01:06,698 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180106-0000/1 is now RUNNING
2025-04-02T18:01:06,699 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180106-0000/0 is now RUNNING
2025-04-02T18:01:06,700 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180106-0000/2 is now RUNNING
2025-04-02T18:01:06,705 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180106-0000.inprogress
2025-04-02T18:01:06,790 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@433931f8{/,null,STOPPED,@Spark}
2025-04-02T18:01:06,791 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@204fcc55{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,792 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6df60a8e{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,794 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6c9f9e14{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,796 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28a24789{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,797 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@b88d6b5{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,798 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36728d6b{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,800 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@dffa44d{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,800 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@713d422e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,801 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7770f17{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,804 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@71f6d05a{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,814 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6938dd00{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,819 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ab6cff0{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,820 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@702f5497{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,822 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c6fe956{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,825 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7bf682ec{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,829 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2b766e1a{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,830 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2abc7dd2{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,832 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4a3217d4{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,834 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@75deb5d9{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,838 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@183a0074{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,839 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c705c44{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,843 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67d4d4d3{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,851 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6745e80a{/static,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,853 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5371b483{/,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,855 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c96da7b{/api,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,856 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@556979be{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,864 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d042690{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,871 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4f8ffd8b{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:06,872 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:01:07,019 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:01:07,022 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:01:07,038 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c78fbab{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:01:07,040 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@748faef4{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:07,046 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@797b7289{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:01:07,050 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@54c35010{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:01:07,053 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@17f8744a{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:01:07,359 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 141@d332fd2e5338
2025-04-02T18:01:07,364 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:01:07,364 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:01:07,364 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:01:07,391 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 140@44b70a14a7a1
2025-04-02T18:01:07,395 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:01:07,396 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:01:07,396 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:01:07,405 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 141@046119187e0d
2025-04-02T18:01:07,409 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:01:07,412 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:01:07,413 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:01:07,551 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:01:07,575 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:01:07,596 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:01:07,612 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:01:07,615 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:01:07,618 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:07,619 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:07,620 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:01:07,647 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:01:07,648 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:01:07,649 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:07,649 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:07,651 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:01:07,663 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:01:07,665 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:01:07,668 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:01:07,669 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:01:07,670 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:01:07,680 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-02T18:01:07,682 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:01:07,689 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@a41bc5b{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:01:07,692 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:01:07,695 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:01:07,698 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:01:07,704 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180106-0000
2025-04-02T18:01:07,705 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180106-0000
2025-04-02T18:01:07,720 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180106-0000/2
2025-04-02T18:01:07,722 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180106-0000/1
2025-04-02T18:01:07,722 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180106-0000/1 interrupted
2025-04-02T18:01:07,723 [ExecutorRunner for app-20250402180106-0000/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:01:07,722 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180106-0000/2 interrupted
2025-04-02T18:01:07,723 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180106-0000/0
2025-04-02T18:01:07,725 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180106-0000/0 interrupted
2025-04-02T18:01:07,724 [ExecutorRunner for app-20250402180106-0000/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:01:07,725 [dispatcher-event-loop-8] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:01:07,726 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:01:07,727 [ExecutorRunner for app-20250402180106-0000/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:01:07,733 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:01:07,738 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:01:07,739 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:01:07,741 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:01:07,743 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:01:07,743 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180106-0000/1 finished with state KILLED exitStatus 143
2025-04-02T18:01:07,744 [dispatcher-event-loop-7] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180106-0000/1
2025-04-02T18:01:07,744 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:01:07,745 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:01:07,745 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180106-0000, execId=1)
2025-04-02T18:01:07,746 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180106-0000
2025-04-02T18:01:07,746 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180106-0000 removed, cleanupLocalDirs = true
2025-04-02T18:01:07,748 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:38360 got disassociated, removing it.
2025-04-02T18:01:07,750 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:43919 got disassociated, removing it.
2025-04-02T18:01:07,751 [dispatcher-event-loop-1] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180106-0000/2
2025-04-02T18:01:07,751 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180106-0000/2 finished with state KILLED exitStatus 143
2025-04-02T18:01:07,752 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:01:07,752 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:01:07,752 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:01:07,753 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180106-0000, execId=2)
2025-04-02T18:01:07,753 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-9b038bed-2c93-4c0a-9d56-61c371b8b801
2025-04-02T18:01:07,754 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180106-0000 removed, cleanupLocalDirs = true
2025-04-02T18:01:07,754 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180106-0000
2025-04-02T18:01:07,756 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c5e4142e-1d80-4c14-bdd6-292f31eacd18
2025-04-02T18:01:07,757 [dispatcher-event-loop-3] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180106-0000/0
2025-04-02T18:01:07,757 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180106-0000/0 finished with state KILLED exitStatus 143
2025-04-02T18:01:07,758 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:01:07,758 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180106-0000, execId=0)
2025-04-02T18:01:07,759 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-c5e4142e-1d80-4c14-bdd6-292f31eacd18/pyspark-e8660dc0-56ff-4680-b514-98584d6a6d0e
2025-04-02T18:01:07,759 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180106-0000 removed, cleanupLocalDirs = true
2025-04-02T18:01:07,760 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180106-0000
2025-04-02T18:03:22,399 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:03:22,402 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:03:22,402 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:03:22,419 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:03:22,420 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:03:22,420 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:03:22,421 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: pr-3-app.py
2025-04-02T18:03:22,438 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:03:22,444 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:03:22,445 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:03:22,479 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:03:22,480 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:03:22,481 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:22,481 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:22,482 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:03:22,519 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:22,632 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 37557.
2025-04-02T18:03:22,646 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:03:22,662 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:03:22,670 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:03:22,670 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:03:22,673 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:03:22,683 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-7b993f10-2216-4a70-82f2-19799999d96b
2025-04-02T18:03:22,690 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:03:22,697 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:03:22,716 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1523ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:03:22,755 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:03:22,760 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:03:22,770 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1577ms
2025-04-02T18:03:22,784 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@4cda23dc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:03:22,784 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:03:22,794 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@50843431{/,null,AVAILABLE,@Spark}
2025-04-02T18:03:22,842 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:03:22,863 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 11 ms (0 ms spent in bootstraps)
2025-04-02T18:03:22,902 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registering app pr-3-app.py
2025-04-02T18:03:22,904 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Registered app pr-3-app.py with ID app-20250402180322-0001
2025-04-02T18:03:22,906 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180322-0001 with rpId: 0
2025-04-02T18:03:22,907 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180322-0001/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:03:22,908 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180322-0001/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:03:22,910 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180322-0001
2025-04-02T18:03:22,910 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180322-0001/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:03:22,914 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180322-0001/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:03:22,918 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180322-0001/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:03:22,919 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180322-0001/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:03:22,920 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180322-0001/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:03:22,921 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180322-0001/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:03:22,920 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180322-0001/0 for pr-3-app.py
2025-04-02T18:03:22,922 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180322-0001/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:03:22,922 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45933.
2025-04-02T18:03:22,922 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180322-0001/1 for pr-3-app.py
2025-04-02T18:03:22,924 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:45933
2025-04-02T18:03:22,924 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180322-0001/2 for pr-3-app.py
2025-04-02T18:03:22,925 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:03:22,927 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:22,928 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:22,928 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:22,928 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:22,928 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:22,929 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:22,929 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:22,929 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:22,929 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:22,929 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:22,929 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:22,930 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:22,930 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:22,930 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 45933, None)
2025-04-02T18:03:22,931 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:22,931 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:22,933 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:45933 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 45933, None)
2025-04-02T18:03:22,936 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 45933, None)
2025-04-02T18:03:22,942 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 45933, None)
2025-04-02T18:03:22,942 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37557" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:37557" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180322-0001" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:03:22,943 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37557" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:37557" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180322-0001" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:03:22,944 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=37557" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:37557" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180322-0001" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:03:22,958 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180322-0001 with rpId: 0
2025-04-02T18:03:22,960 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180322-0001 with rpId: 0
2025-04-02T18:03:22,961 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180322-0001 with rpId: 0
2025-04-02T18:03:22,971 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180322-0001/0 is now RUNNING
2025-04-02T18:03:22,973 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180322-0001/2 is now RUNNING
2025-04-02T18:03:22,973 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180322-0001/1 is now RUNNING
2025-04-02T18:03:23,092 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180322-0001.inprogress
2025-04-02T18:03:23,176 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@50843431{/,null,STOPPED,@Spark}
2025-04-02T18:03:23,178 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58bc9feb{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,178 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@27d11237{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,180 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@493e64be{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,182 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9e01cea{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,183 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@596d8226{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,184 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6267033{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,185 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@304fe3c2{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,186 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@17fae08f{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,188 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@52ba5ce6{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,190 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e1684f{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,196 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3f49f40d{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,199 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c31a685{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,200 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65362d28{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,202 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@660d7ad{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,202 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d7b2dac{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,204 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@188541dc{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,205 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15f84b61{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,205 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7e46acf1{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,206 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@8ec214f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,209 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@73b1a218{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,210 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ebf880{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,210 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@188cea1b{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,215 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@c859cac{/static,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,216 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4ba19ef5{/,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,218 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@109d8a80{/api,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,219 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40a9b0d8{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,220 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ec3ed53{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,223 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37419c2f{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,224 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:03:23,354 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:03:23,355 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:03:23,374 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c3ed25b{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,375 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7942be7d{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,379 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65be0fab{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,381 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@514ededa{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,384 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@77ce3fbc{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:03:23,648 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 179@046119187e0d
2025-04-02T18:03:23,655 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:03:23,655 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 182@44b70a14a7a1
2025-04-02T18:03:23,656 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:03:23,656 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:03:23,659 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:03:23,658 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 183@d332fd2e5338
2025-04-02T18:03:23,659 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:03:23,659 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:03:23,662 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:03:23,663 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:03:23,663 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:03:23,848 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:23,849 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:23,856 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:23,896 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:23,897 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:23,897 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:23,897 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:23,898 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:23,900 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:23,898 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:23,901 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:23,902 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:23,902 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:23,902 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:23,901 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:23,906 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:23,906 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:23,906 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:23,955 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-02T18:03:23,955 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:03:23,963 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@4cda23dc{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:03:23,965 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:03:23,967 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:03:23,970 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:03:23,981 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180322-0001
2025-04-02T18:03:23,982 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180322-0001
2025-04-02T18:03:23,983 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180322-0001/2
2025-04-02T18:03:23,984 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180322-0001/2 interrupted
2025-04-02T18:03:23,984 [ExecutorRunner for app-20250402180322-0001/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:03:23,986 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180322-0001/1
2025-04-02T18:03:23,987 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180322-0001/1 interrupted
2025-04-02T18:03:23,986 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180322-0001/0
2025-04-02T18:03:23,988 [ExecutorRunner for app-20250402180322-0001/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:03:23,989 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180322-0001/0 interrupted
2025-04-02T18:03:23,990 [ExecutorRunner for app-20250402180322-0001/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:03:23,990 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:03:23,993 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:03:23,995 [dispatcher-event-loop-7] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:03:23,995 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:03:24,005 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:03:24,005 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:03:24,005 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180322-0001/2 finished with state KILLED exitStatus 143
2025-04-02T18:03:24,005 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180322-0001/0 finished with state KILLED exitStatus 143
2025-04-02T18:03:24,006 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:03:24,006 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:03:24,007 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180322-0001/0
2025-04-02T18:03:24,007 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180322-0001, execId=2)
2025-04-02T18:03:24,007 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180322-0001, execId=0)
2025-04-02T18:03:24,007 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180322-0001/2
2025-04-02T18:03:24,007 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180322-0001 removed, cleanupLocalDirs = true
2025-04-02T18:03:24,007 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180322-0001
2025-04-02T18:03:24,008 [dispatcher-event-loop-3] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180322-0001 removed, cleanupLocalDirs = true
2025-04-02T18:03:24,008 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180322-0001
2025-04-02T18:03:24,010 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:03:24,011 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:03:24,013 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180322-0001/1 finished with state KILLED exitStatus 143
2025-04-02T18:03:24,013 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:33604 got disassociated, removing it.
2025-04-02T18:03:24,014 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:03:24,014 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:37557 got disassociated, removing it.
2025-04-02T18:03:24,014 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180322-0001, execId=1)
2025-04-02T18:03:24,014 [dispatcher-event-loop-6] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180322-0001/1
2025-04-02T18:03:24,015 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180322-0001 removed, cleanupLocalDirs = true
2025-04-02T18:03:24,015 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180322-0001
2025-04-02T18:03:24,017 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:03:24,017 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:03:24,018 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-47f27144-fb99-4bdb-b93b-0c4c79350c2e
2025-04-02T18:03:24,020 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-47f27144-fb99-4bdb-b93b-0c4c79350c2e/pyspark-de9a1832-2003-4684-a8d7-22a522cbc044
2025-04-02T18:03:24,022 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ebb8d861-f41a-423f-ad21-e40357ee121b
2025-04-02T18:03:31,633 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:03:31,635 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:03:31,636 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:03:31,647 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:03:31,647 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:03:31,648 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:03:31,648 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: pr-7-app.py
2025-04-02T18:03:31,658 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:03:31,662 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:03:31,665 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:03:31,687 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:03:31,687 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:03:31,688 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:31,688 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:31,688 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:03:31,712 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:31,816 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 45287.
2025-04-02T18:03:31,827 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:03:31,840 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:03:31,847 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:03:31,847 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:03:31,850 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:03:31,859 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-9efb9998-31c4-45ca-82d8-55d153ec88b0
2025-04-02T18:03:31,865 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:03:31,871 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:03:31,888 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1165ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:03:31,925 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:03:31,930 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:03:31,938 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1216ms
2025-04-02T18:03:31,952 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@57ba3f76{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:03:31,952 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:03:31,961 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@31f764c{/,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,008 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:03:32,028 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 11 ms (0 ms spent in bootstraps)
2025-04-02T18:03:32,062 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Registering app pr-7-app.py
2025-04-02T18:03:32,063 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Registered app pr-7-app.py with ID app-20250402180332-0002
2025-04-02T18:03:32,064 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180332-0002 with rpId: 0
2025-04-02T18:03:32,064 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180332-0002/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:03:32,065 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180332-0002/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:03:32,066 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180332-0002/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:03:32,067 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180332-0002
2025-04-02T18:03:32,068 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180332-0002/1 for pr-7-app.py
2025-04-02T18:03:32,068 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180332-0002/0 for pr-7-app.py
2025-04-02T18:03:32,069 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180332-0002/2 for pr-7-app.py
2025-04-02T18:03:32,069 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180332-0002/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:03:32,070 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:32,070 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:32,070 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:32,070 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:32,070 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:32,071 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:32,070 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:32,071 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:32,071 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:32,071 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:32,071 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:32,071 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180332-0002/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:03:32,072 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:32,072 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:32,072 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180332-0002/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:03:32,073 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41423.
2025-04-02T18:03:32,073 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180332-0002/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:03:32,073 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:32,074 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:41423
2025-04-02T18:03:32,074 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:32,074 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180332-0002/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:03:32,075 [dispatcher-event-loop-7] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180332-0002/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:03:32,076 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:03:32,077 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45287" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:45287" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180332-0002" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:03:32,079 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 41423, None)
2025-04-02T18:03:32,079 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45287" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:45287" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180332-0002" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:03:32,081 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45287" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:45287" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180332-0002" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:03:32,082 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180332-0002 with rpId: 0
2025-04-02T18:03:32,084 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:41423 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 41423, None)
2025-04-02T18:03:32,088 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 41423, None)
2025-04-02T18:03:32,088 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180332-0002 with rpId: 0
2025-04-02T18:03:32,090 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 41423, None)
2025-04-02T18:03:32,091 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180332-0002 with rpId: 0
2025-04-02T18:03:32,095 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180332-0002/1 is now RUNNING
2025-04-02T18:03:32,098 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180332-0002/2 is now RUNNING
2025-04-02T18:03:32,099 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180332-0002/0 is now RUNNING
2025-04-02T18:03:32,213 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180332-0002.inprogress
2025-04-02T18:03:32,283 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@31f764c{/,null,STOPPED,@Spark}
2025-04-02T18:03:32,287 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5d06af77{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,293 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1f109a67{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,297 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a306e05{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,298 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@294f5d69{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,299 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@71b0d394{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,301 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7286b2a0{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,301 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@103a43f3{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,303 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@727e176e{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,304 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3d2e86c0{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,305 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@60141746{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,306 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1d008176{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,308 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@228a1def{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,308 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@67388543{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,310 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@53cba7df{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,316 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5b7db998{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,317 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@11851fa5{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,318 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3e1d13f9{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,319 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d7e6f83{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,320 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@653e65ea{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,321 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5664f42a{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,323 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@223ef189{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,325 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5f38bf40{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,329 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7082d7a1{/static,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,330 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@307714a2{/,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,331 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@35aacc01{/api,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,335 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7db162c7{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,335 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@66218062{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,342 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@38f1e9a3{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,343 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:03:32,478 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:03:32,482 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:03:32,489 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3b589934{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,493 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e55912d{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,499 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6dccebd1{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,502 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@514f8e55{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,508 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@8a5a303{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:03:32,689 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 219@046119187e0d
2025-04-02T18:03:32,693 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:03:32,696 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:03:32,697 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:03:32,697 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 223@44b70a14a7a1
2025-04-02T18:03:32,701 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:03:32,702 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:03:32,702 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:03:32,705 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 223@d332fd2e5338
2025-04-02T18:03:32,709 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:03:32,711 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:03:32,713 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:03:32,874 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:32,898 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:32,903 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:03:32,931 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:32,933 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:32,934 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:32,936 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:32,937 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:32,965 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:32,965 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:03:32,965 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:32,966 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:32,966 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:03:32,967 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:32,967 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:03:32,967 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:32,967 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:03:32,968 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:03:33,072 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-02T18:03:33,073 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:03:33,079 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@57ba3f76{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:03:33,082 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:03:33,085 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:03:33,087 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:03:33,092 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180332-0002
2025-04-02T18:03:33,092 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180332-0002
2025-04-02T18:03:33,094 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180332-0002/0
2025-04-02T18:03:33,095 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180332-0002/1
2025-04-02T18:03:33,095 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180332-0002/0 interrupted
2025-04-02T18:03:33,096 [ExecutorRunner for app-20250402180332-0002/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:03:33,099 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180332-0002/2
2025-04-02T18:03:33,096 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180332-0002/1 interrupted
2025-04-02T18:03:33,101 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:03:33,102 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180332-0002/2 interrupted
2025-04-02T18:03:33,103 [ExecutorRunner for app-20250402180332-0002/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:03:33,102 [ExecutorRunner for app-20250402180332-0002/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:03:33,107 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:03:33,107 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:03:33,120 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45287 after 36 ms (0 ms spent in bootstraps)
2025-04-02T18:03:33,124 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180332-0002/0 finished with state KILLED exitStatus 143
2025-04-02T18:03:33,125 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:03:33,125 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180332-0002, execId=0)
2025-04-02T18:03:33,126 [dispatcher-event-loop-4] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180332-0002 removed, cleanupLocalDirs = true
2025-04-02T18:03:33,126 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180332-0002
2025-04-02T18:03:33,126 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180332-0002/0
2025-04-02T18:03:33,127 [dispatcher-event-loop-7] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:03:33,133 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180332-0002/1 finished with state KILLED exitStatus 143
2025-04-02T18:03:33,133 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:03:33,134 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180332-0002, execId=1)
2025-04-02T18:03:33,134 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:03:33,134 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180332-0002 removed, cleanupLocalDirs = true
2025-04-02T18:03:33,134 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180332-0002
2025-04-02T18:03:33,134 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:03:33,134 [dispatcher-event-loop-3] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180332-0002/1
2025-04-02T18:03:33,137 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:03:33,138 [dispatcher-event-loop-3] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:03:33,140 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:39362 got disassociated, removing it.
2025-04-02T18:03:33,140 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:45287 got disassociated, removing it.
2025-04-02T18:03:33,142 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:03:33,143 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:03:33,143 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-1c61d243-0f77-40bb-bedb-cc5fddea6202
2025-04-02T18:03:33,144 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-9713a118-9d52-40b6-a575-247d5d0ea152
2025-04-02T18:03:33,146 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-9713a118-9d52-40b6-a575-247d5d0ea152/pyspark-958bc6ce-1a6e-42c8-9f07-526dcb025f72
2025-04-02T18:03:33,153 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180332-0002/2 finished with state KILLED exitStatus 143
2025-04-02T18:03:33,153 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:03:33,153 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180332-0002, execId=2)
2025-04-02T18:03:33,153 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180332-0002/2
2025-04-02T18:03:33,154 [dispatcher-event-loop-2] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180332-0002 removed, cleanupLocalDirs = true
2025-04-02T18:03:33,154 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180332-0002
2025-04-02T18:06:10,846 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:06:10,849 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:06:10,849 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:06:10,861 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:10,862 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:06:10,862 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:10,862 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: get-users-json.py
2025-04-02T18:06:10,871 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:06:10,875 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:06:10,876 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:06:10,900 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:06:10,900 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:06:10,901 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:10,901 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:10,902 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:06:10,928 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:11,030 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 43485.
2025-04-02T18:06:11,044 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:06:11,060 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:06:11,070 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:06:11,071 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:06:11,073 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:06:11,083 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-2483c384-d6f4-4e22-9bcd-e1ecc4e9a7ef
2025-04-02T18:06:11,092 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:06:11,104 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:06:11,125 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1334ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:06:11,170 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:06:11,175 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:06:11,186 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1395ms
2025-04-02T18:06:11,200 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@61885633{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:06:11,200 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:06:11,211 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58239a30{/,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,259 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:06:11,286 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 12 ms (0 ms spent in bootstraps)
2025-04-02T18:06:11,329 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app get-users-json.py
2025-04-02T18:06:11,336 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app get-users-json.py with ID app-20250402180611-0003
2025-04-02T18:06:11,337 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180611-0003 with rpId: 0
2025-04-02T18:06:11,338 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180611-0003/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:06:11,339 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180611-0003/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:06:11,340 [dispatcher-event-loop-1] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180611-0003
2025-04-02T18:06:11,341 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180611-0003/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:06:11,341 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180611-0003/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:06:11,347 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34273.
2025-04-02T18:06:11,348 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:34273
2025-04-02T18:06:11,348 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180611-0003/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:06:11,349 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180611-0003/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:06:11,350 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:06:11,348 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180611-0003/0 for get-users-json.py
2025-04-02T18:06:11,350 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180611-0003/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:06:11,351 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180611-0003/1 for get-users-json.py
2025-04-02T18:06:11,352 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180611-0003/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:06:11,352 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180611-0003/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:06:11,353 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180611-0003/2 for get-users-json.py
2025-04-02T18:06:11,354 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 34273, None)
2025-04-02T18:06:11,359 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:34273 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 34273, None)
2025-04-02T18:06:11,360 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:11,360 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:11,361 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:11,361 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:11,361 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 34273, None)
2025-04-02T18:06:11,362 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:11,362 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:11,362 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:11,362 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:11,362 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:11,363 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:11,363 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 34273, None)
2025-04-02T18:06:11,363 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:11,363 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:11,364 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:11,364 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:11,364 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:11,377 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43485" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:43485" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180611-0003" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:06:11,377 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43485" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:43485" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180611-0003" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:06:11,378 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=43485" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:43485" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180611-0003" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:06:11,392 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180611-0003 with rpId: 0
2025-04-02T18:06:11,393 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180611-0003 with rpId: 0
2025-04-02T18:06:11,396 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180611-0003 with rpId: 0
2025-04-02T18:06:11,405 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180611-0003/0 is now RUNNING
2025-04-02T18:06:11,408 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180611-0003/2 is now RUNNING
2025-04-02T18:06:11,408 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180611-0003/1 is now RUNNING
2025-04-02T18:06:11,510 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180611-0003.inprogress
2025-04-02T18:06:11,584 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@58239a30{/,null,STOPPED,@Spark}
2025-04-02T18:06:11,585 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5539a0c3{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,586 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5defa822{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,588 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@566d5c32{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,590 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@72ee2564{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,591 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22f24c9f{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,593 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6e3344b0{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,597 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@646fb514{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,598 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6ed649b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,599 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3c1b906e{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,600 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2fe6fc81{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,602 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@40628859{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,605 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@ee34860{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,606 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d5cde90{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,607 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@62ac5ef6{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,608 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1556dbd6{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,609 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2666f06e{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,610 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@72b95835{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,610 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f315dac{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,611 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@53aab329{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,612 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@617d0638{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,614 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5d2c2989{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,615 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37f7f50a{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,622 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ff7047b{/static,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,623 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@541126c0{/,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,625 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@64459334{/api,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,626 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7ed727f3{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,627 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@a6a958c{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,630 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@58f1d8d8{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,631 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:06:11,731 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:06:11,733 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:06:11,740 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5450e848{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,743 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d5cb762{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,745 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7784187b{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,747 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5434014{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:11,750 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@57544828{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:06:12,065 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 260@046119187e0d
2025-04-02T18:06:12,066 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 265@44b70a14a7a1
2025-04-02T18:06:12,070 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:06:12,071 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:06:12,071 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:06:12,071 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:06:12,071 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:06:12,075 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:06:12,077 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 265@d332fd2e5338
2025-04-02T18:06:12,082 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:06:12,083 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:06:12,085 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:06:12,240 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:12,242 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:12,260 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:12,287 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:12,288 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:12,289 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:12,289 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:12,289 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:12,291 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:12,292 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:12,292 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:12,293 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:12,294 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:12,332 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:12,335 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:12,336 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:12,336 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:12,337 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:12,411 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-02T18:06:12,412 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:06:12,416 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@61885633{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:06:12,418 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:06:12,420 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:06:12,422 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:06:12,427 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180611-0003
2025-04-02T18:06:12,427 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180611-0003
2025-04-02T18:06:12,429 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180611-0003/0
2025-04-02T18:06:12,430 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180611-0003/0 interrupted
2025-04-02T18:06:12,432 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180611-0003/1
2025-04-02T18:06:12,432 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180611-0003/2
2025-04-02T18:06:12,443 [ExecutorRunner for app-20250402180611-0003/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:06:12,444 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180611-0003/1 interrupted
2025-04-02T18:06:12,444 [ExecutorRunner for app-20250402180611-0003/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:06:12,444 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180611-0003/2 interrupted
2025-04-02T18:06:12,445 [ExecutorRunner for app-20250402180611-0003/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:06:12,445 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:06:12,447 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:06:12,447 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:06:12,447 [dispatcher-event-loop-9] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:06:12,455 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180611-0003/1 finished with state KILLED exitStatus 143
2025-04-02T18:06:12,455 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:06:12,456 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180611-0003, execId=1)
2025-04-02T18:06:12,456 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180611-0003/2 finished with state KILLED exitStatus 143
2025-04-02T18:06:12,456 [dispatcher-event-loop-7] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180611-0003/1
2025-04-02T18:06:12,456 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180611-0003 removed, cleanupLocalDirs = true
2025-04-02T18:06:12,456 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:06:12,456 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180611-0003
2025-04-02T18:06:12,457 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180611-0003, execId=2)
2025-04-02T18:06:12,457 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180611-0003/2
2025-04-02T18:06:12,457 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:06:12,457 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180611-0003 removed, cleanupLocalDirs = true
2025-04-02T18:06:12,457 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180611-0003
2025-04-02T18:06:12,457 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:06:12,461 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:06:12,462 [dispatcher-event-loop-5] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:06:12,464 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:45912 got disassociated, removing it.
2025-04-02T18:06:12,464 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:43485 got disassociated, removing it.
2025-04-02T18:06:12,466 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180611-0003/0 finished with state KILLED exitStatus 143
2025-04-02T18:06:12,467 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:06:12,466 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:06:12,467 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180611-0003, execId=0)
2025-04-02T18:06:12,467 [dispatcher-event-loop-6] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180611-0003/0
2025-04-02T18:06:12,467 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:06:12,467 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180611-0003 removed, cleanupLocalDirs = true
2025-04-02T18:06:12,467 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180611-0003
2025-04-02T18:06:12,468 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-767b2b4e-1d60-4a80-8a75-6c6059bfc75c
2025-04-02T18:06:12,470 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-0c21b2bd-a300-4b6c-8ee9-979182141868/pyspark-4c36fd79-f687-4b3b-a132-99afd357b571
2025-04-02T18:06:12,471 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-0c21b2bd-a300-4b6c-8ee9-979182141868
2025-04-02T18:06:30,490 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:06:30,492 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:06:30,492 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:06:30,503 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:30,504 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:06:30,504 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:30,504 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: get-users-json.py
2025-04-02T18:06:30,513 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:06:30,517 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:06:30,517 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:06:30,539 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:06:30,539 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:06:30,539 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:30,540 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:30,540 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:06:30,564 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:30,658 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 44931.
2025-04-02T18:06:30,669 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:06:30,683 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:06:30,688 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:06:30,689 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:06:30,691 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:06:30,700 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-50d3d762-4e18-41c4-920c-2c28458e4693
2025-04-02T18:06:30,706 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:06:30,712 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:06:30,729 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1145ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:06:30,764 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:06:30,769 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:06:30,777 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1193ms
2025-04-02T18:06:30,790 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@4f8e1256{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:06:30,790 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:06:30,801 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5c67f40a{/,null,AVAILABLE,@Spark}
2025-04-02T18:06:30,845 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:06:30,871 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 12 ms (0 ms spent in bootstraps)
2025-04-02T18:06:30,903 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registering app get-users-json.py
2025-04-02T18:06:30,904 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Registered app get-users-json.py with ID app-20250402180630-0004
2025-04-02T18:06:30,905 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180630-0004 with rpId: 0
2025-04-02T18:06:30,905 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180630-0004/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:06:30,906 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180630-0004/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:06:30,907 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180630-0004/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:06:30,908 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180630-0004/0 for get-users-json.py
2025-04-02T18:06:30,909 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180630-0004
2025-04-02T18:06:30,910 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180630-0004/1 for get-users-json.py
2025-04-02T18:06:30,910 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180630-0004/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:06:30,910 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180630-0004/2 for get-users-json.py
2025-04-02T18:06:30,911 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:30,912 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180630-0004/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:30,912 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:30,913 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:30,913 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180630-0004/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:06:30,913 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:30,913 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:30,914 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:30,914 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:30,914 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:30,914 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180630-0004/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:06:30,914 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:30,915 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180630-0004/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:06:30,915 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180630-0004/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:06:30,915 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42739.
2025-04-02T18:06:30,915 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:42739
2025-04-02T18:06:30,916 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:06:30,919 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44931" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:44931" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180630-0004" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:06:30,920 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44931" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:44931" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180630-0004" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:06:30,921 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=44931" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:44931" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180630-0004" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:06:30,920 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 42739, None)
2025-04-02T18:06:30,923 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:42739 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 42739, None)
2025-04-02T18:06:30,925 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180630-0004 with rpId: 0
2025-04-02T18:06:30,926 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180630-0004 with rpId: 0
2025-04-02T18:06:30,927 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 42739, None)
2025-04-02T18:06:30,929 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 42739, None)
2025-04-02T18:06:30,930 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180630-0004 with rpId: 0
2025-04-02T18:06:30,935 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180630-0004/0 is now RUNNING
2025-04-02T18:06:30,936 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180630-0004/2 is now RUNNING
2025-04-02T18:06:30,938 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180630-0004/1 is now RUNNING
2025-04-02T18:06:31,047 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180630-0004.inprogress
2025-04-02T18:06:31,113 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@5c67f40a{/,null,STOPPED,@Spark}
2025-04-02T18:06:31,115 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@57cc3671{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,117 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1c367d68{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,118 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@37d131e1{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,119 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4f2bac0c{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,121 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@77fbaf70{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,121 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@63791aae{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,124 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fc08a16{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,125 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6acf727b{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,126 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@687cf944{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,129 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@33f32e88{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,130 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@157ccab8{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,132 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6bdbcb6d{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,133 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2f40d684{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,135 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6a264a67{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,137 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@481f600a{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,139 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@547744f6{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,141 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@15993baa{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,142 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@25b0cf90{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,143 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7791e6e0{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,144 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6cdf01a2{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,146 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@710fea89{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,146 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2eee0f86{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,152 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@24b3699a{/static,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,153 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4a4c8b46{/,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,155 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7d58bf08{/api,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,156 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2e781b46{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,156 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@179a21d2{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,158 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@dcc8bdc{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,160 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:06:31,318 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:06:31,320 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:06:31,327 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@376753bc{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,328 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@61871f8{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,332 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2ff0545d{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,337 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@e28ddf3{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,339 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@65db7d0e{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:06:31,500 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 302@046119187e0d
2025-04-02T18:06:31,504 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:06:31,505 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:06:31,506 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:06:31,527 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 305@d332fd2e5338
2025-04-02T18:06:31,531 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:06:31,531 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:06:31,531 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:06:31,532 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 307@44b70a14a7a1
2025-04-02T18:06:31,536 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:06:31,537 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:06:31,537 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:06:31,690 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:31,699 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:31,710 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:06:31,723 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 19 ms to list leaf files for 1 paths.
2025-04-02T18:06:31,740 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:31,740 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:31,743 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:31,743 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:31,744 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:31,744 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:31,744 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:31,744 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:31,744 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:31,745 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:31,758 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 2 ms to list leaf files for 1 paths.
2025-04-02T18:06:31,761 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:31,761 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:31,764 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:31,765 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:31,767 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:31,949 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:44931 after 27 ms (0 ms spent in bootstraps)
2025-04-02T18:06:31,954 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:44931 after 31 ms (0 ms spent in bootstraps)
2025-04-02T18:06:31,964 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:44931 after 25 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,020 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:32,020 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:32,021 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:32,021 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:32,021 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:32,022 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:32,022 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:32,023 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:32,023 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:32,023 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:32,023 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:06:32,024 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:06:32,025 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:06:32,025 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:06:32,025 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:06:32,056 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:44931 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,068 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:44931 after 8 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,070 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:44931 after 3 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,107 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-ff6e6ccd-2eba-41e9-9cb2-1191e12d2a96/executor-185c936c-451b-4b67-b00f-1e0c1775d6ac/blockmgr-9d6149e9-654c-4bbf-adb0-26d3a984af5a
2025-04-02T18:06:32,110 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-e157e6e8-8f40-4d96-851c-cefa427a2493/executor-4b9c6bb0-a4ec-4818-b0cd-e3a8860b3183/blockmgr-7ff54f10-b487-4702-b6d6-7019952982d7
2025-04-02T18:06:32,111 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-cc0a9846-f448-4700-b9cc-18627fb6d92e/executor-77209791-be5d-4543-958f-c6260e453f70/blockmgr-e0adc9f9-f239-4cf1-bc79-d1047d1e87e2
2025-04-02T18:06:32,126 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:06:32,129 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:06:32,130 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:06:32,250 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@eff501c685cd:44931
2025-04-02T18:06:32,250 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:37899
2025-04-02T18:06:32,253 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:37899 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,255 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:37899
2025-04-02T18:06:32,255 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@eff501c685cd:44931
2025-04-02T18:06:32,256 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:35613
2025-04-02T18:06:32,256 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@eff501c685cd:44931
2025-04-02T18:06:32,257 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:45421
2025-04-02T18:06:32,260 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:32,261 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-02T18:06:32,261 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:45421 after 2 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,261 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:32,262 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:35613 after 2 ms (0 ms spent in bootstraps)
2025-04-02T18:06:32,265 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:32,265 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-02T18:06:32,266 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:45421
2025-04-02T18:06:32,266 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:35613
2025-04-02T18:06:32,268 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:32,272 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:32,273 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-02T18:06:32,278 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:06:32,290 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35906) with ID 2,  ResourceProfileId 0
2025-04-02T18:06:32,293 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:59844) with ID 0,  ResourceProfileId 0
2025-04-02T18:06:32,297 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-02T18:06:32,298 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-02T18:06:32,300 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.3
2025-04-02T18:06:32,300 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:06:32,301 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.4
2025-04-02T18:06:32,301 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-02T18:06:32,306 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:06:32,306 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-02T18:06:32,310 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:43226) with ID 1,  ResourceProfileId 0
2025-04-02T18:06:32,313 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-02T18:06:32,315 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.5
2025-04-02T18:06:32,316 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:06:32,316 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-02T18:06:32,329 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38785.
2025-04-02T18:06:32,329 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:38785
2025-04-02T18:06:32,334 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:06:32,337 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38717.
2025-04-02T18:06:32,337 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:38717
2025-04-02T18:06:32,338 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.3, 38785, None)
2025-04-02T18:06:32,339 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:06:32,342 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.4, 38717, None)
2025-04-02T18:06:32,344 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37847.
2025-04-02T18:06:32,344 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:37847
2025-04-02T18:06:32,347 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:06:32,348 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:38785 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.3, 38785, None)
2025-04-02T18:06:32,349 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:38717 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 38717, None)
2025-04-02T18:06:32,350 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.3, 38785, None)
2025-04-02T18:06:32,351 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.3, 38785, None)
2025-04-02T18:06:32,352 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.5, 37847, None)
2025-04-02T18:06:32,352 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.4, 38717, None)
2025-04-02T18:06:32,354 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.4, 38717, None)
2025-04-02T18:06:32,356 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-02T18:06:32,357 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@724e0d57 for default.
2025-04-02T18:06:32,357 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:37847 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.5, 37847, None)
2025-04-02T18:06:32,360 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-02T18:06:32,360 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.5, 37847, None)
2025-04-02T18:06:32,360 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@75368616 for default.
2025-04-02T18:06:32,361 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.5, 37847, None)
2025-04-02T18:06:32,368 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-02T18:06:32,369 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@61c2a24 for default.
2025-04-02T18:06:32,833 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-02T18:06:32,834 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-02T18:06:32,910 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.2 KiB, free 434.2 MiB)
2025-04-02T18:06:32,932 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-02T18:06:32,934 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on eff501c685cd:42739 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:32,936 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-02T18:06:32,942 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-02T18:06:32,999 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-02T18:06:33,014 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-02T18:06:33,015 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-02T18:06:33,015 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-02T18:06:33,016 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:06:33,018 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-02T18:06:33,053 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-02T18:06:33,054 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-02T18:06:33,055 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on eff501c685cd:42739 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,056 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:06:33,064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:06:33,064 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-02T18:06:33,076 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9610 bytes) 
2025-04-02T18:06:33,087 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-02T18:06:33,091 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-02T18:06:33,136 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:33,158 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:42739 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:06:33,183 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-02T18:06:33,185 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:38785 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,192 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 55 ms
2025-04-02T18:06:33,217 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-02T18:06:33,354 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/users.json, range: 0-330, partition values: [empty row]
2025-04-02T18:06:33,472 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 71.843417 ms
2025-04-02T18:06:33,473 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:33,478 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-02T18:06:33,479 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:38785 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,480 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 6 ms
2025-04-02T18:06:33,509 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-02T18:06:33,568 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2292 bytes result sent to driver
2025-04-02T18:06:33,575 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 503 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-02T18:06:33,576 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-02T18:06:33,578 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 0.553 s
2025-04-02T18:06:33,580 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-02T18:06:33,580 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-02T18:06:33,582 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 0.578822 s
2025-04-02T18:06:33,644 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-02T18:06:33,644 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-02T18:06:33,785 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on eff501c685cd:42739 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,788 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.3:38785 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,798 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on eff501c685cd:42739 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,800 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 87.721459 ms
2025-04-02T18:06:33,803 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.3:38785 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,804 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-02T18:06:33,809 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)
2025-04-02T18:06:33,810 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on eff501c685cd:42739 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,811 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-02T18:06:33,813 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-02T18:06:33,828 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-02T18:06:33,831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-02T18:06:33,831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-02T18:06:33,831 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-02T18:06:33,832 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:06:33,832 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-02T18:06:33,840 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)
2025-04-02T18:06:33,841 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.1 MiB)
2025-04-02T18:06:33,842 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on eff501c685cd:42739 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,842 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:06:33,843 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:06:33,844 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-02T18:06:33,845 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9599 bytes) 
2025-04-02T18:06:33,850 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-02T18:06:33,856 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-02T18:06:33,897 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:33,916 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:42739 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:06:33,947 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-02T18:06:33,949 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.4:38717 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:06:33,951 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 53 ms
2025-04-02T18:06:33,972 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-02T18:06:34,214 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 80.737208 ms
2025-04-02T18:06:34,219 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/users.json, range: 0-330, partition values: [empty row]
2025-04-02T18:06:34,237 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 6.422667 ms
2025-04-02T18:06:34,246 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:34,251 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-02T18:06:34,253 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.4:38717 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,254 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 7 ms
2025-04-02T18:06:34,278 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-02T18:06:34,361 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 2007 bytes result sent to driver
2025-04-02T18:06:34,366 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 521 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-02T18:06:34,366 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-02T18:06:34,367 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 0.533 s
2025-04-02T18:06:34,368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-02T18:06:34,368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-02T18:06:34,368 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-02T18:06:34,369 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-02T18:06:34,392 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 8.911584 ms
2025-04-02T18:06:34,404 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-02T18:06:34,405 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-02T18:06:34,405 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-02T18:06:34,406 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-02T18:06:34,406 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:06:34,406 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-02T18:06:34,410 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
2025-04-02T18:06:34,416 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.1 MiB)
2025-04-02T18:06:34,417 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on eff501c685cd:42739 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,418 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on eff501c685cd:42739 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,418 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:06:34,419 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:06:34,419 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-02T18:06:34,422 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.4:38717 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,422 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.4, executor 0, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-02T18:06:34,425 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-02T18:06:34,425 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-02T18:06:34,429 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-02T18:06:34,435 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:34,439 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-02T18:06:34,440 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.4:38717 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,441 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 6 ms
2025-04-02T18:06:34,442 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-02T18:06:34,467 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-02T18:06:34,468 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@eff501c685cd:44931)
2025-04-02T18:06:34,471 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.4:59844
2025-04-02T18:06:34,491 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-02T18:06:34,503 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-02T18:06:34,504 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 6 ms
2025-04-02T18:06:34,515 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 8.203542 ms
2025-04-02T18:06:34,521 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4038 bytes result sent to driver
2025-04-02T18:06:34,524 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 102 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-02T18:06:34,525 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-02T18:06:34,526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.116 s
2025-04-02T18:06:34,526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-02T18:06:34,526 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-02T18:06:34,526 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.122079 s
2025-04-02T18:06:34,560 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-02T18:06:34,560 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-02T18:06:34,593 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.715709 ms
2025-04-02T18:06:34,596 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)
2025-04-02T18:06:34,603 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)
2025-04-02T18:06:34,604 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on eff501c685cd:42739 (size: 34.5 KiB, free: 434.3 MiB)
2025-04-02T18:06:34,605 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from showString at <unknown>:0
2025-04-02T18:06:34,605 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on eff501c685cd:42739 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,606 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-02T18:06:34,607 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_2_piece0 on 172.18.0.4:38717 in memory (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,612 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on eff501c685cd:42739 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,614 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-02T18:06:34,615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (showString at <unknown>:0) with 1 output partitions
2025-04-02T18:06:34,615 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.4:38717 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (showString at <unknown>:0)
2025-04-02T18:06:34,615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-02T18:06:34,615 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:06:34,616 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[14] at showString at <unknown>:0), which has no missing parents
2025-04-02T18:06:34,618 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 18.0 KiB, free 434.2 MiB)
2025-04-02T18:06:34,620 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.1 MiB)
2025-04-02T18:06:34,621 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on eff501c685cd:42739 (size: 7.8 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,621 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:06:34,622 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:06:34,622 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-02T18:06:34,626 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.4, executor 0, partition 0, PROCESS_LOCAL, 9610 bytes) 
2025-04-02T18:06:34,628 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-02T18:06:34,628 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-02T18:06:34,630 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:34,634 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.4 MiB)
2025-04-02T18:06:34,635 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.4:38717 (size: 7.8 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,636 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 5 ms
2025-04-02T18:06:34,637 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 18.0 KiB, free 434.4 MiB)
2025-04-02T18:06:34,655 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.0715 ms
2025-04-02T18:06:34,657 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/users.json, range: 0-330, partition values: [empty row]
2025-04-02T18:06:34,698 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 20.454625 ms
2025-04-02T18:06:34,700 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:06:34,704 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.3 MiB)
2025-04-02T18:06:34,705 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 172.18.0.4:38717 (size: 34.5 KiB, free: 434.4 MiB)
2025-04-02T18:06:34,706 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 5 took 5 ms
2025-04-02T18:06:34,710 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-02T18:06:34,718 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 4.0 (TID 3). 1792 bytes result sent to driver
2025-04-02T18:06:34,720 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 4.0 (TID 3) in 94 ms on 172.18.0.4 (executor 0) (1/1)
2025-04-02T18:06:34,720 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-02T18:06:34,721 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (showString at <unknown>:0) finished in 0.104 s
2025-04-02T18:06:34,721 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-02T18:06:34,721 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage finished
2025-04-02T18:06:34,721 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 finished: showString at <unknown>:0, took 0.107379 s
2025-04-02T18:06:34,750 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 20.709541 ms
2025-04-02T18:06:34,754 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:06:34,757 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@4f8e1256{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:06:34,759 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:06:34,760 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:06:34,761 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:06:34,764 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-02T18:06:34,764 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-02T18:06:34,766 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180630-0004
2025-04-02T18:06:34,767 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180630-0004
2025-04-02T18:06:34,768 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-02T18:06:34,769 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180630-0004/2
2025-04-02T18:06:34,769 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180630-0004/2 interrupted
2025-04-02T18:06:34,769 [ExecutorRunner for app-20250402180630-0004/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:06:34,770 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180630-0004/0
2025-04-02T18:06:34,770 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180630-0004/1
2025-04-02T18:06:34,772 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:06:34,772 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180630-0004/0 interrupted
2025-04-02T18:06:34,774 [ExecutorRunner for app-20250402180630-0004/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:06:34,774 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180630-0004/1 interrupted
2025-04-02T18:06:34,775 [ExecutorRunner for app-20250402180630-0004/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:06:34,776 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:06:34,777 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:06:34,778 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:06:34,778 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:06:34,779 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:06:34,779 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:06:34,779 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:06:34,780 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:06:34,779 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:06:34,783 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:06:34,783 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:06:34,784 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:06:34,792 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:06:34,793 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:06:34,795 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:06:34,796 [dispatcher-event-loop-4] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:06:34,797 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180630-0004/1 finished with state KILLED exitStatus 143
2025-04-02T18:06:34,797 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:06:34,797 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180630-0004, execId=1)
2025-04-02T18:06:34,797 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180630-0004 removed, cleanupLocalDirs = true
2025-04-02T18:06:34,797 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180630-0004/1
2025-04-02T18:06:34,797 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180630-0004
2025-04-02T18:06:34,798 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180630-0004/0 finished with state KILLED exitStatus 143
2025-04-02T18:06:34,799 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:06:34,799 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180630-0004, execId=0)
2025-04-02T18:06:34,799 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180630-0004/0
2025-04-02T18:06:34,799 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180630-0004 removed, cleanupLocalDirs = true
2025-04-02T18:06:34,799 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180630-0004
2025-04-02T18:06:34,799 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:46582 got disassociated, removing it.
2025-04-02T18:06:34,800 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:44931 got disassociated, removing it.
2025-04-02T18:06:34,803 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:06:34,805 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180630-0004/2 finished with state KILLED exitStatus 143
2025-04-02T18:06:34,806 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180630-0004/2
2025-04-02T18:06:34,806 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:06:34,806 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180630-0004, execId=2)
2025-04-02T18:06:34,806 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180630-0004
2025-04-02T18:06:34,806 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180630-0004 removed, cleanupLocalDirs = true
2025-04-02T18:06:35,239 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:06:35,240 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-829bbcee-f9a5-4b94-bbd2-953e0f4156b4/pyspark-9f74fbac-880f-48de-af79-dcac8de7e8ae
2025-04-02T18:06:35,243 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-578ce5ec-aa7b-4a57-80b9-0bfe2e809f46
2025-04-02T18:06:35,245 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-829bbcee-f9a5-4b94-bbd2-953e0f4156b4
2025-04-02T18:07:32,970 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:07:32,972 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:07:32,972 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:07:32,984 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:32,985 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:07:32,985 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:32,986 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: get-users-json.py
2025-04-02T18:07:32,995 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:07:33,000 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:07:33,000 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:07:33,023 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:07:33,024 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:07:33,024 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:33,024 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:33,025 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:07:33,051 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:33,155 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 35873.
2025-04-02T18:07:33,166 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:07:33,181 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:07:33,189 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:07:33,189 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:07:33,191 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:07:33,201 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-2a7377d6-ff1c-4531-bf2c-d06adafbb4ad
2025-04-02T18:07:33,206 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:07:33,214 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:07:33,232 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1259ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:07:33,272 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:07:33,278 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:07:33,287 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1314ms
2025-04-02T18:07:33,299 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@65e23aa9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:07:33,299 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:07:33,309 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@539ffc40{/,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,355 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:07:33,377 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 12 ms (0 ms spent in bootstraps)
2025-04-02T18:07:33,413 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registering app get-users-json.py
2025-04-02T18:07:33,415 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Registered app get-users-json.py with ID app-20250402180733-0005
2025-04-02T18:07:33,416 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180733-0005 with rpId: 0
2025-04-02T18:07:33,417 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180733-0005/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:07:33,418 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180733-0005/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:07:33,419 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180733-0005/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:07:33,420 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180733-0005
2025-04-02T18:07:33,424 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180733-0005/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:07:33,425 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180733-0005/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:07:33,426 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180733-0005/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:07:33,426 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180733-0005/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:07:33,427 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180733-0005/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:07:33,427 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180733-0005/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:07:33,427 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180733-0005/0 for get-users-json.py
2025-04-02T18:07:33,428 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180733-0005/1 for get-users-json.py
2025-04-02T18:07:33,428 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180733-0005/2 for get-users-json.py
2025-04-02T18:07:33,429 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42343.
2025-04-02T18:07:33,430 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:42343
2025-04-02T18:07:33,432 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:07:33,432 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:33,433 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:33,434 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:33,434 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:33,434 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:33,434 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:33,434 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:33,436 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 42343, None)
2025-04-02T18:07:33,438 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:42343 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 42343, None)
2025-04-02T18:07:33,440 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 42343, None)
2025-04-02T18:07:33,441 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 42343, None)
2025-04-02T18:07:33,445 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35873" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:35873" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180733-0005" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:07:33,445 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35873" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:35873" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180733-0005" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:07:33,446 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=35873" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:35873" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180733-0005" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:07:33,455 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180733-0005 with rpId: 0
2025-04-02T18:07:33,456 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180733-0005 with rpId: 0
2025-04-02T18:07:33,457 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180733-0005 with rpId: 0
2025-04-02T18:07:33,464 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180733-0005/0 is now RUNNING
2025-04-02T18:07:33,465 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180733-0005/2 is now RUNNING
2025-04-02T18:07:33,466 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180733-0005/1 is now RUNNING
2025-04-02T18:07:33,573 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180733-0005.inprogress
2025-04-02T18:07:33,649 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@539ffc40{/,null,STOPPED,@Spark}
2025-04-02T18:07:33,650 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@60d0f7d4{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,652 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@61b86a7{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,653 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7de63259{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,654 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@53b1a9bc{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,655 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2d1c20c9{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,658 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@26ff0da0{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,659 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1a3be03a{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,661 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@208bbc77{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,662 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5cce0bc0{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,662 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fe7d8b0{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,663 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@426c15c3{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,664 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@144ef782{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,665 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5c0a7859{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,665 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6b18629{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,668 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@22753b02{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,669 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@49c179e5{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,671 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7dd8c6d8{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,672 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@253e4db5{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,673 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1ae004fa{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,673 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3a96cf5d{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,674 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2e30c046{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,675 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5c192f46{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,680 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2e9bd9bb{/static,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,681 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3978e224{/,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,683 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1a8e0874{/api,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,683 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@76faa2ae{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,686 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@71570216{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,689 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@8eaf871{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,690 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:07:33,825 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:07:33,828 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:07:33,855 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@44767910{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,856 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@9090635{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,858 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@36f019be{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,859 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@272f49e2{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:33,865 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@54ec32e6{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:07:34,129 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 401@44b70a14a7a1
2025-04-02T18:07:34,134 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:07:34,134 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 396@d332fd2e5338
2025-04-02T18:07:34,135 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:07:34,135 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:07:34,138 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:07:34,139 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:07:34,139 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:07:34,145 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 397@046119187e0d
2025-04-02T18:07:34,153 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:07:34,153 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:07:34,153 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:07:34,324 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:34,342 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:34,348 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:34,379 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:34,381 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:34,381 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:34,382 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:34,382 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:34,382 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:34,382 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:34,385 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:34,385 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:34,385 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:34,395 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:34,397 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:34,398 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:34,398 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:34,398 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:34,509 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Invoking stop() from shutdown hook
2025-04-02T18:07:34,510 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:07:34,514 [shutdown-hook-0] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@65e23aa9{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:07:34,516 [shutdown-hook-0] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:07:34,518 [shutdown-hook-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:07:34,520 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:07:34,527 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180733-0005
2025-04-02T18:07:34,528 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180733-0005
2025-04-02T18:07:34,531 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180733-0005/1
2025-04-02T18:07:34,531 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180733-0005/1 interrupted
2025-04-02T18:07:34,532 [ExecutorRunner for app-20250402180733-0005/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:07:34,534 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180733-0005/0
2025-04-02T18:07:34,534 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180733-0005/2
2025-04-02T18:07:34,535 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180733-0005/0 interrupted
2025-04-02T18:07:34,538 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180733-0005/2 interrupted
2025-04-02T18:07:34,535 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:07:34,538 [ExecutorRunner for app-20250402180733-0005/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:07:34,538 [ExecutorRunner for app-20250402180733-0005/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:07:34,541 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:07:34,544 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:07:34,546 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180733-0005/1 finished with state KILLED exitStatus 143
2025-04-02T18:07:34,547 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180733-0005/1
2025-04-02T18:07:34,547 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:07:34,548 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180733-0005, execId=1)
2025-04-02T18:07:34,546 [dispatcher-event-loop-5] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:07:34,548 [dispatcher-event-loop-9] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180733-0005 removed, cleanupLocalDirs = true
2025-04-02T18:07:34,548 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180733-0005
2025-04-02T18:07:34,551 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180733-0005/2 finished with state KILLED exitStatus 143
2025-04-02T18:07:34,552 [dispatcher-event-loop-0] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180733-0005/2
2025-04-02T18:07:34,553 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:07:34,553 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180733-0005, execId=2)
2025-04-02T18:07:34,553 [dispatcher-event-loop-7] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180733-0005 removed, cleanupLocalDirs = true
2025-04-02T18:07:34,553 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180733-0005
2025-04-02T18:07:34,554 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180733-0005/0 finished with state KILLED exitStatus 143
2025-04-02T18:07:34,554 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:07:34,555 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180733-0005, execId=0)
2025-04-02T18:07:34,555 [dispatcher-event-loop-8] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180733-0005 removed, cleanupLocalDirs = true
2025-04-02T18:07:34,555 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180733-0005
2025-04-02T18:07:34,556 [dispatcher-event-loop-5] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180733-0005/0
2025-04-02T18:07:34,557 [shutdown-hook-0] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:07:34,559 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:07:34,562 [shutdown-hook-0] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:07:34,563 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:07:34,565 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:46436 got disassociated, removing it.
2025-04-02T18:07:34,565 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:35873 got disassociated, removing it.
2025-04-02T18:07:34,567 [shutdown-hook-0] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:07:34,567 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:07:34,567 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-d161def9-b2d8-4b08-b365-15ae2c4c0f6d
2025-04-02T18:07:34,569 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-02cacce1-20d8-425a-be4a-2cff35627c5e
2025-04-02T18:07:34,571 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-d161def9-b2d8-4b08-b365-15ae2c4c0f6d/pyspark-250d84bd-88b5-48aa-8150-3677c3c95a76
2025-04-02T18:07:54,212 [Thread-4] INFO  org.apache.spark.SparkContext [] - Running Spark version 3.5.5
2025-04-02T18:07:54,214 [Thread-4] INFO  org.apache.spark.SparkContext [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:07:54,215 [Thread-4] INFO  org.apache.spark.SparkContext [] - Java version 17.0.14
2025-04-02T18:07:54,226 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:54,227 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.driver.
2025-04-02T18:07:54,227 [Thread-4] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:54,227 [Thread-4] INFO  org.apache.spark.SparkContext [] - Submitted application: get-users-json.py
2025-04-02T18:07:54,237 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
2025-04-02T18:07:54,241 [Thread-4] INFO  org.apache.spark.resource.ResourceProfile [] - Limiting resource is cpu
2025-04-02T18:07:54,241 [Thread-4] INFO  org.apache.spark.resource.ResourceProfileManager [] - Added ResourceProfile id: 0
2025-04-02T18:07:54,263 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: root,spark
2025-04-02T18:07:54,263 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: root,spark
2025-04-02T18:07:54,263 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:54,264 [Thread-4] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:54,264 [Thread-4] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
2025-04-02T18:07:54,287 [Thread-4] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:54,385 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'sparkDriver' on port 45475.
2025-04-02T18:07:54,397 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering MapOutputTracker
2025-04-02T18:07:54,413 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMaster
2025-04-02T18:07:54,421 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2025-04-02T18:07:54,422 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - BlockManagerMasterEndpoint up
2025-04-02T18:07:54,424 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering BlockManagerMasterHeartbeat
2025-04-02T18:07:54,433 [Thread-4] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/blockmgr-5af3e33c-2979-4a64-b085-3d8b8ba1a4ad
2025-04-02T18:07:54,439 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:07:54,445 [Thread-4] INFO  org.apache.spark.SparkEnv [] - Registering OutputCommitCoordinator
2025-04-02T18:07:54,462 [Thread-4] INFO  org.sparkproject.jetty.util.log [] - Logging initialized @1192ms to org.sparkproject.jetty.util.log.Slf4jLog
2025-04-02T18:07:54,498 [Thread-4] INFO  org.apache.spark.ui.JettyUtils [] - Start Jetty 0.0.0.0:4040 for SparkUI
2025-04-02T18:07:54,504 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.14+10-LTS
2025-04-02T18:07:54,514 [Thread-4] INFO  org.sparkproject.jetty.server.Server [] - Started @1244ms
2025-04-02T18:07:54,528 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Started ServerConnector@2d6fb4e7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:07:54,528 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'SparkUI' on port 4040.
2025-04-02T18:07:54,536 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7a029960{/,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,583 [appclient-register-master-threadpool-0] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Connecting to master spark://spark-master:7077...
2025-04-02T18:07:54,603 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to spark-master/172.18.0.2:7077 after 11 ms (0 ms spent in bootstraps)
2025-04-02T18:07:54,635 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Registering app get-users-json.py
2025-04-02T18:07:54,635 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Registered app get-users-json.py with ID app-20250402180754-0006
2025-04-02T18:07:54,636 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180754-0006 with rpId: 0
2025-04-02T18:07:54,637 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180754-0006/0 on worker worker-20250402174822-172.18.0.4-37899
2025-04-02T18:07:54,637 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180754-0006/1 on worker worker-20250402174822-172.18.0.5-45421
2025-04-02T18:07:54,637 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.master.Master [] - Launching executor app-20250402180754-0006/2 on worker worker-20250402174822-172.18.0.3-35613
2025-04-02T18:07:54,639 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180754-0006/0 for get-users-json.py
2025-04-02T18:07:54,639 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180754-0006/1 for get-users-json.py
2025-04-02T18:07:54,641 [dispatcher-event-loop-0] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Connected to Spark cluster with app ID app-20250402180754-0006
2025-04-02T18:07:54,641 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to launch executor app-20250402180754-0006/2 for get-users-json.py
2025-04-02T18:07:54,642 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180754-0006/0 on worker-20250402174822-172.18.0.4-37899 (172.18.0.4:37899) with 2 core(s)
2025-04-02T18:07:54,643 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:54,643 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:54,643 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:54,645 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:54,645 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:54,645 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:54,645 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:54,646 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:54,645 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:54,646 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:54,646 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180754-0006/0 on hostPort 172.18.0.4:37899 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:07:54,646 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:54,646 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:54,647 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:54,647 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:54,647 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180754-0006/1 on worker-20250402174822-172.18.0.5-45421 (172.18.0.5:45421) with 2 core(s)
2025-04-02T18:07:54,647 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:54,647 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180754-0006/1 on hostPort 172.18.0.5:45421 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:07:54,648 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor added: app-20250402180754-0006/2 on worker-20250402174822-172.18.0.3-35613 (172.18.0.3:35613) with 2 core(s)
2025-04-02T18:07:54,648 [Thread-4] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41391.
2025-04-02T18:07:54,648 [dispatcher-event-loop-9] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Granted executor ID app-20250402180754-0006/2 on hostPort 172.18.0.3:35613 with 2 core(s), 1024.0 MiB RAM
2025-04-02T18:07:54,648 [Thread-4] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on eff501c685cd:41391
2025-04-02T18:07:54,650 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:07:54,652 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45475" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:45475" "--executor-id" "1" "--hostname" "172.18.0.5" "--cores" "2" "--app-id" "app-20250402180754-0006" "--worker-url" "spark://Worker@172.18.0.5:45421" "--resourceProfileId" "0"
2025-04-02T18:07:54,653 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45475" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:45475" "--executor-id" "2" "--hostname" "172.18.0.3" "--cores" "2" "--app-id" "app-20250402180754-0006" "--worker-url" "spark://Worker@172.18.0.3:35613" "--resourceProfileId" "0"
2025-04-02T18:07:54,653 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Launch command: "/opt/bitnami/java/bin/java" "-cp" "/opt/bitnami/spark/conf/:/opt/bitnami/spark/jars/*" "-Xmx1024M" "-Dspark.driver.port=45475" "-Djava.net.preferIPv6Addresses=false" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/jdk.internal.ref=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "-Djdk.reflect.useDirectMethodHandle=false" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@eff501c685cd:45475" "--executor-id" "0" "--hostname" "172.18.0.4" "--cores" "2" "--app-id" "app-20250402180754-0006" "--worker-url" "spark://Worker@172.18.0.4:37899" "--resourceProfileId" "0"
2025-04-02T18:07:54,653 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(driver, eff501c685cd, 41391, None)
2025-04-02T18:07:54,656 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager eff501c685cd:41391 with 434.4 MiB RAM, BlockManagerId(driver, eff501c685cd, 41391, None)
2025-04-02T18:07:54,659 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(driver, eff501c685cd, 41391, None)
2025-04-02T18:07:54,660 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(driver, eff501c685cd, 41391, None)
2025-04-02T18:07:54,661 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180754-0006 with rpId: 0
2025-04-02T18:07:54,662 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180754-0006 with rpId: 0
2025-04-02T18:07:54,663 [dispatcher-event-loop-3] INFO  org.apache.spark.deploy.master.Master [] - Start scheduling for app app-20250402180754-0006 with rpId: 0
2025-04-02T18:07:54,675 [dispatcher-event-loop-6] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180754-0006/0 is now RUNNING
2025-04-02T18:07:54,679 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180754-0006/1 is now RUNNING
2025-04-02T18:07:54,680 [dispatcher-event-loop-2] INFO  org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint [] - Executor updated: app-20250402180754-0006/2 is now RUNNING
2025-04-02T18:07:54,779 [Thread-4] INFO  org.apache.spark.deploy.history.SingleEventLogFileWriter [] - Logging events to file:/opt/bitnami/spark/logs/events/app-20250402180754-0006.inprogress
2025-04-02T18:07:54,840 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Stopped o.s.j.s.ServletContextHandler@7a029960{/,null,STOPPED,@Spark}
2025-04-02T18:07:54,841 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2acd7263{/jobs,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,842 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5f23ea9{/jobs/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,843 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@10e99963{/jobs/job,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,843 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@28f96c39{/jobs/job/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,844 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@27ef9ee5{/stages,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,844 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@48f9d941{/stages/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,846 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1d89af34{/stages/stage,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,846 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@34dcd04d{/stages/stage/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,848 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@b6b420d{/stages/pool,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,849 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@483a64b4{/stages/pool/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,850 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@1fef02cd{/storage,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,853 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2afb4b51{/storage/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,854 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@29150a00{/storage/rdd,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,854 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@14c6585a{/storage/rdd/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,855 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7183dc5{/environment,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,857 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7c426914{/environment/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,858 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5fdb8827{/executors,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,859 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6df7eae7{/executors/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,859 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2c01dd4f{/executors/threadDump,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,860 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6db98d82{/executors/threadDump/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,861 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@2711fde5{/executors/heapHistogram,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,863 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@660cb821{/executors/heapHistogram/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,867 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@edfdd26{/static,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,868 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@33ce8065{/,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,869 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5e9f7be6{/api,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,871 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@5b985fc6{/jobs/job/kill,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,873 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@208f2863{/stages/stage/kill,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,875 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@4e45511c{/metrics/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,876 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
2025-04-02T18:07:54,980 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
2025-04-02T18:07:54,981 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState [] - Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
2025-04-02T18:07:54,990 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@19159886{/SQL,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,991 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@6d38e816{/SQL/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,991 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@49702a30{/SQL/execution,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,994 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@3ccf9e44{/SQL/execution/json,null,AVAILABLE,@Spark}
2025-04-02T18:07:54,996 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler [] - Started o.s.j.s.ServletContextHandler@7fdc37b9{/static/sql,null,AVAILABLE,@Spark}
2025-04-02T18:07:55,188 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 443@44b70a14a7a1
2025-04-02T18:07:55,192 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:07:55,193 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:07:55,193 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:07:55,215 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 438@d332fd2e5338
2025-04-02T18:07:55,220 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:07:55,220 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:07:55,221 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:07:55,233 [main] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Started daemon with process name: 439@046119187e0d
2025-04-02T18:07:55,238 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for TERM
2025-04-02T18:07:55,239 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for HUP
2025-04-02T18:07:55,240 [main] INFO  org.apache.spark.util.SignalUtils [] - Registering signal handler for INT
2025-04-02T18:07:55,343 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:55,367 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 18 ms to list leaf files for 1 paths.
2025-04-02T18:07:55,379 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:55,379 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:55,380 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:55,380 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:55,381 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:55,392 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:55,400 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex [] - It took 1 ms to list leaf files for 1 paths.
2025-04-02T18:07:55,434 [main] WARN  org.apache.hadoop.util.NativeCodeLoader [] - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2025-04-02T18:07:55,455 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:55,456 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:55,460 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:55,460 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:55,461 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:55,472 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:55,473 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:55,474 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:55,475 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:55,475 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:55,516 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45475 after 30 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,571 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:55,572 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:55,572 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:55,574 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:55,574 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:55,599 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45475 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,613 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45475 after 30 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,622 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45475 after 26 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,637 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-ff6e6ccd-2eba-41e9-9cb2-1191e12d2a96/executor-a9713250-7430-4986-a8fe-945a896bb315/blockmgr-62244883-4ca5-471f-830b-12a67a303710
2025-04-02T18:07:55,656 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:55,656 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:55,657 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:55,657 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:55,657 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:07:55,658 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:55,676 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls to: spark
2025-04-02T18:07:55,676 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls to: spark
2025-04-02T18:07:55,676 [main] INFO  org.apache.spark.SecurityManager [] - Changing view acls groups to: 
2025-04-02T18:07:55,677 [main] INFO  org.apache.spark.SecurityManager [] - Changing modify acls groups to: 
2025-04-02T18:07:55,677 [main] INFO  org.apache.spark.SecurityManager [] - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
2025-04-02T18:07:55,682 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45475 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,704 [netty-rpc-connection-0] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:45475 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,725 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-cc0a9846-f448-4700-b9cc-18627fb6d92e/executor-bf622320-d481-43e7-a3e9-85bcd01d5d1c/blockmgr-0eb8f020-00af-4e2c-a813-24e5c139410d
2025-04-02T18:07:55,739 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.3:35613
2025-04-02T18:07:55,738 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@eff501c685cd:45475
2025-04-02T18:07:55,741 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:07:55,747 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:55,747 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.3:35613 after 3 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,747 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-02T18:07:55,748 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:55,750 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.3:35613
2025-04-02T18:07:55,751 [main] INFO  org.apache.spark.storage.DiskBlockManager [] - Created local directory at /tmp/spark-e157e6e8-8f40-4d96-851c-cefa427a2493/executor-30b6be87-c475-45d9-8893-b2ea11f358c3/blockmgr-3999a9b7-a4c3-4f59-9c0c-2148156c2c3f
2025-04-02T18:07:55,774 [main] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore started with capacity 434.4 MiB
2025-04-02T18:07:55,782 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:35630) with ID 2,  ResourceProfileId 0
2025-04-02T18:07:55,786 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-02T18:07:55,788 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 2 on host 172.18.0.3
2025-04-02T18:07:55,790 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:07:55,791 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-02T18:07:55,813 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35523.
2025-04-02T18:07:55,813 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.3:35523
2025-04-02T18:07:55,817 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:07:55,820 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(2, 172.18.0.3, 35523, None)
2025-04-02T18:07:55,830 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.3:35523 with 434.4 MiB RAM, BlockManagerId(2, 172.18.0.3, 35523, None)
2025-04-02T18:07:55,833 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(2, 172.18.0.3, 35523, None)
2025-04-02T18:07:55,834 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(2, 172.18.0.3, 35523, None)
2025-04-02T18:07:55,848 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-02T18:07:55,849 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1d685f37 for default.
2025-04-02T18:07:55,866 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@eff501c685cd:45475
2025-04-02T18:07:55,866 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.5:45421
2025-04-02T18:07:55,882 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:55,882 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-02T18:07:55,883 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:55,885 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.5:45421 after 3 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,888 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.5:45421
2025-04-02T18:07:55,895 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:60224) with ID 1,  ResourceProfileId 0
2025-04-02T18:07:55,898 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-02T18:07:55,899 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 1 on host 172.18.0.5
2025-04-02T18:07:55,900 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:07:55,900 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-02T18:07:55,907 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Connecting to driver: spark://CoarseGrainedScheduler@eff501c685cd:45475
2025-04-02T18:07:55,907 [main] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Connecting to worker spark://Worker@172.18.0.4:37899
2025-04-02T18:07:55,910 [netty-rpc-connection-1] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to /172.18.0.4:37899 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:07:55,912 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.WorkerWatcher [] - Successfully connected to spark://Worker@172.18.0.4:37899
2025-04-02T18:07:55,913 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:55,914 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - No custom resources configured for spark.executor.
2025-04-02T18:07:55,914 [dispatcher-Executor] INFO  org.apache.spark.resource.ResourceUtils [] - ==============================================================
2025-04-02T18:07:55,920 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42067.
2025-04-02T18:07:55,921 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.5:42067
2025-04-02T18:07:55,922 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:07:55,925 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:39910) with ID 0,  ResourceProfileId 0
2025-04-02T18:07:55,925 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(1, 172.18.0.5, 42067, None)
2025-04-02T18:07:55,927 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Successfully registered with driver
2025-04-02T18:07:55,929 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor ID 0 on host 172.18.0.4
2025-04-02T18:07:55,929 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - OS info Linux, 6.10.14-linuxkit, aarch64
2025-04-02T18:07:55,930 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Java version 17.0.14
2025-04-02T18:07:55,931 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.5:42067 with 434.4 MiB RAM, BlockManagerId(1, 172.18.0.5, 42067, None)
2025-04-02T18:07:55,932 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(1, 172.18.0.5, 42067, None)
2025-04-02T18:07:55,933 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(1, 172.18.0.5, 42067, None)
2025-04-02T18:07:55,937 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-02T18:07:55,938 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3470d7cb for default.
2025-04-02T18:07:55,948 [dispatcher-Executor] INFO  org.apache.spark.util.Utils [] - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33939.
2025-04-02T18:07:55,949 [dispatcher-Executor] INFO  org.apache.spark.network.netty.NettyBlockTransferService [] - Server created on 172.18.0.4:33939
2025-04-02T18:07:55,950 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2025-04-02T18:07:55,953 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registering BlockManager BlockManagerId(0, 172.18.0.4, 33939, None)
2025-04-02T18:07:55,956 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerMasterEndpoint [] - Registering block manager 172.18.0.4:33939 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 33939, None)
2025-04-02T18:07:55,959 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManagerMaster [] - Registered BlockManager BlockManagerId(0, 172.18.0.4, 33939, None)
2025-04-02T18:07:55,960 [dispatcher-Executor] INFO  org.apache.spark.storage.BlockManager [] - Initialized BlockManager: BlockManagerId(0, 172.18.0.4, 33939, None)
2025-04-02T18:07:55,963 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Starting executor with user classpath (userClassPathFirst = false): ''
2025-04-02T18:07:55,964 [dispatcher-Executor] INFO  org.apache.spark.executor.Executor [] - Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3bef9167 for default.
2025-04-02T18:07:56,422 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-02T18:07:56,423 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-02T18:07:56,496 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 200.2 KiB, free 434.2 MiB)
2025-04-02T18:07:56,520 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-02T18:07:56,522 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on eff501c685cd:41391 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-02T18:07:56,525 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 0 from json at <unknown>:0
2025-04-02T18:07:56,531 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-02T18:07:56,590 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: json at <unknown>:0
2025-04-02T18:07:56,600 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 0 (json at <unknown>:0) with 1 output partitions
2025-04-02T18:07:56,601 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 0 (json at <unknown>:0)
2025-04-02T18:07:56,601 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-02T18:07:56,602 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:07:56,604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0), which has no missing parents
2025-04-02T18:07:56,638 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.2 MiB)
2025-04-02T18:07:56,641 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.1 MiB)
2025-04-02T18:07:56,642 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on eff501c685cd:41391 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:07:56,642 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 1 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:07:56,650 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at json at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:07:56,651 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 0.0 with 1 tasks resource profile 0
2025-04-02T18:07:56,663 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9610 bytes) 
2025-04-02T18:07:56,676 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 0
2025-04-02T18:07:56,682 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 0.0 (TID 0)
2025-04-02T18:07:56,725 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 1 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:56,743 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.network.client.TransportClientFactory [] - Successfully created connection to eff501c685cd/172.18.0.2:41391 after 1 ms (0 ms spent in bootstraps)
2025-04-02T18:07:56,762 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.4 MiB)
2025-04-02T18:07:56,765 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_1_piece0 in memory on 172.18.0.3:35523 (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:07:56,767 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 1 took 41 ms
2025-04-02T18:07:56,798 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_1 stored as values in memory (estimated size 16.5 KiB, free 434.4 MiB)
2025-04-02T18:07:56,941 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/users.json, range: 0-330, partition values: [empty row]
2025-04-02T18:07:57,073 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 76.049667 ms
2025-04-02T18:07:57,075 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 0 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:57,079 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-02T18:07:57,080 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_0_piece0 in memory on 172.18.0.3:35523 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,081 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 0 took 6 ms
2025-04-02T18:07:57,109 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_0 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-02T18:07:57,162 [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 0.0 (TID 0). 2292 bytes result sent to driver
2025-04-02T18:07:57,170 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 0.0 (TID 0) in 511 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-02T18:07:57,171 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 0.0, whose tasks have all completed, from pool 
2025-04-02T18:07:57,173 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 0 (json at <unknown>:0) finished in 0.563 s
2025-04-02T18:07:57,175 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-02T18:07:57,175 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 0: Stage finished
2025-04-02T18:07:57,177 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 0 finished: json at <unknown>:0, took 0.586668 s
2025-04-02T18:07:57,243 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-02T18:07:57,243 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-02T18:07:57,306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on eff501c685cd:41391 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,309 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_1_piece0 on 172.18.0.3:35523 in memory (size: 7.7 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,318 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on eff501c685cd:41391 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,318 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_0_piece0 on 172.18.0.3:35523 in memory (size: 34.4 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,404 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 83.111875 ms
2025-04-02T18:07:57,406 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 200.1 KiB, free 434.2 MiB)
2025-04-02T18:07:57,410 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.2 MiB)
2025-04-02T18:07:57,411 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on eff501c685cd:41391 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,411 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 2 from count at <unknown>:0
2025-04-02T18:07:57,413 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-02T18:07:57,428 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Registering RDD 7 (count at <unknown>:0) as input to shuffle 0
2025-04-02T18:07:57,430 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got map stage job 1 (count at <unknown>:0) with 1 output partitions
2025-04-02T18:07:57,431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ShuffleMapStage 1 (count at <unknown>:0)
2025-04-02T18:07:57,431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-02T18:07:57,431 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:07:57,432 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0), which has no missing parents
2025-04-02T18:07:57,439 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.2 MiB)
2025-04-02T18:07:57,440 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.1 MiB)
2025-04-02T18:07:57,441 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on eff501c685cd:41391 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,441 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 3 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:07:57,444 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:07:57,445 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 1.0 with 1 tasks resource profile 0
2025-04-02T18:07:57,446 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9599 bytes) 
2025-04-02T18:07:57,449 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 1
2025-04-02T18:07:57,449 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 1.0 (TID 1)
2025-04-02T18:07:57,463 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 3 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:57,467 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 434.4 MiB)
2025-04-02T18:07:57,468 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_3_piece0 in memory on 172.18.0.3:35523 (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,470 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 3 took 6 ms
2025-04-02T18:07:57,471 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 434.4 MiB)
2025-04-02T18:07:57,503 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 10.699292 ms
2025-04-02T18:07:57,508 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/users.json, range: 0-330, partition values: [empty row]
2025-04-02T18:07:57,513 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 3.943 ms
2025-04-02T18:07:57,519 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 2 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:57,523 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 434.3 MiB)
2025-04-02T18:07:57,524 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_2_piece0 in memory on 172.18.0.3:35523 (size: 34.4 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,525 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 2 took 5 ms
2025-04-02T18:07:57,530 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_2 stored as values in memory (estimated size 377.3 KiB, free 434.0 MiB)
2025-04-02T18:07:57,552 [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 1.0 (TID 1). 1964 bytes result sent to driver
2025-04-02T18:07:57,557 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 1.0 (TID 1) in 111 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-02T18:07:57,558 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 1.0, whose tasks have all completed, from pool 
2025-04-02T18:07:57,562 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ShuffleMapStage 1 (count at <unknown>:0) finished in 0.129 s
2025-04-02T18:07:57,563 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - looking for newly runnable stages
2025-04-02T18:07:57,564 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - running: Set()
2025-04-02T18:07:57,564 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - waiting: Set()
2025-04-02T18:07:57,565 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - failed: Set()
2025-04-02T18:07:57,589 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 8.994166 ms
2025-04-02T18:07:57,601 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: count at <unknown>:0
2025-04-02T18:07:57,602 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 2 (count at <unknown>:0) with 1 output partitions
2025-04-02T18:07:57,603 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 3 (count at <unknown>:0)
2025-04-02T18:07:57,603 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List(ShuffleMapStage 2)
2025-04-02T18:07:57,603 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:07:57,604 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0), which has no missing parents
2025-04-02T18:07:57,608 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.1 MiB)
2025-04-02T18:07:57,613 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.1 MiB)
2025-04-02T18:07:57,614 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on eff501c685cd:41391 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,615 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on eff501c685cd:41391 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,615 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 4 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:07:57,615 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_3_piece0 on 172.18.0.3:35523 in memory (size: 8.3 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,616 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at count at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:07:57,616 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 3.0 with 1 tasks resource profile 0
2025-04-02T18:07:57,619 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 3.0 (TID 2) (172.18.0.3, executor 2, partition 0, NODE_LOCAL, 9003 bytes) 
2025-04-02T18:07:57,621 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 2
2025-04-02T18:07:57,622 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 3.0 (TID 2)
2025-04-02T18:07:57,624 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Updating epoch to 1 and clearing cache
2025-04-02T18:07:57,625 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 4 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:57,636 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.0 MiB)
2025-04-02T18:07:57,637 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_4_piece0 in memory on 172.18.0.3:35523 (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,638 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 4 took 9 ms
2025-04-02T18:07:57,639 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_4 stored as values in memory (estimated size 12.5 KiB, free 434.0 MiB)
2025-04-02T18:07:57,646 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Don't have map outputs for shuffle 0, fetching them
2025-04-02T18:07:57,647 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@eff501c685cd:45475)
2025-04-02T18:07:57,649 [dispatcher-event-loop-1] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - Asked to send map output locations for shuffle 0 to 172.18.0.3:35630
2025-04-02T18:07:57,669 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.MapOutputTrackerWorker [] - Got the map output locations
2025-04-02T18:07:57,683 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
2025-04-02T18:07:57,684 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.storage.ShuffleBlockFetcherIterator [] - Started 0 remote fetches in 6 ms
2025-04-02T18:07:57,694 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 7.298959 ms
2025-04-02T18:07:57,701 [Executor task launch worker for task 0.0 in stage 3.0 (TID 2)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 3.0 (TID 2). 4038 bytes result sent to driver
2025-04-02T18:07:57,704 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 3.0 (TID 2) in 85 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-02T18:07:57,704 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 3.0, whose tasks have all completed, from pool 
2025-04-02T18:07:57,705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 3 (count at <unknown>:0) finished in 0.098 s
2025-04-02T18:07:57,705 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-02T18:07:57,706 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 3: Stage finished
2025-04-02T18:07:57,706 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 2 finished: count at <unknown>:0, took 0.104839 s
2025-04-02T18:07:57,741 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Pushed Filters: 
2025-04-02T18:07:57,742 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy [] - Post-Scan Filters: 
2025-04-02T18:07:57,774 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 14.150208 ms
2025-04-02T18:07:57,776 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 200.1 KiB, free 434.0 MiB)
2025-04-02T18:07:57,782 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on eff501c685cd:41391 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,782 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-02T18:07:57,783 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on eff501c685cd:41391 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-02T18:07:57,783 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Removed broadcast_4_piece0 on 172.18.0.3:35523 in memory (size: 6.0 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,783 [Thread-4] INFO  org.apache.spark.SparkContext [] - Created broadcast 5 from showString at <unknown>:0
2025-04-02T18:07:57,784 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec [] - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
2025-04-02T18:07:57,791 [Thread-4] INFO  org.apache.spark.SparkContext [] - Starting job: showString at <unknown>:0
2025-04-02T18:07:57,792 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Got job 3 (showString at <unknown>:0) with 1 output partitions
2025-04-02T18:07:57,792 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Final stage: ResultStage 4 (showString at <unknown>:0)
2025-04-02T18:07:57,792 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Parents of final stage: List()
2025-04-02T18:07:57,792 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Missing parents: List()
2025-04-02T18:07:57,793 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting ResultStage 4 (MapPartitionsRDD[14] at showString at <unknown>:0), which has no missing parents
2025-04-02T18:07:57,794 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 18.0 KiB, free 433.9 MiB)
2025-04-02T18:07:57,797 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 433.9 MiB)
2025-04-02T18:07:57,798 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on eff501c685cd:41391 (size: 7.8 KiB, free: 434.3 MiB)
2025-04-02T18:07:57,801 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext [] - Created broadcast 6 from broadcast at DAGScheduler.scala:1585
2025-04-02T18:07:57,802 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at showString at <unknown>:0) (first 15 tasks are for partitions Vector(0))
2025-04-02T18:07:57,802 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Adding task set 4.0 with 1 tasks resource profile 0
2025-04-02T18:07:57,803 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager [] - Starting task 0.0 in stage 4.0 (TID 3) (172.18.0.3, executor 2, partition 0, PROCESS_LOCAL, 9610 bytes) 
2025-04-02T18:07:57,805 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Got assigned task 3
2025-04-02T18:07:57,805 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Running task 0.0 in stage 4.0 (TID 3)
2025-04-02T18:07:57,807 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 6 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:57,811 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6_piece0 stored as bytes in memory (estimated size 7.8 KiB, free 434.0 MiB)
2025-04-02T18:07:57,812 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_6_piece0 in memory on 172.18.0.3:35523 (size: 7.8 KiB, free: 434.4 MiB)
2025-04-02T18:07:57,813 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 6 took 5 ms
2025-04-02T18:07:57,814 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_6 stored as values in memory (estimated size 18.0 KiB, free 434.0 MiB)
2025-04-02T18:07:57,840 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 23.661833 ms
2025-04-02T18:07:57,840 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.execution.datasources.FileScanRDD [] - Reading File path: file:///opt/bitnami/spark/storage/users.json, range: 0-330, partition values: [empty row]
2025-04-02T18:07:57,860 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 11.66675 ms
2025-04-02T18:07:57,861 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Started reading broadcast variable 5 with 1 pieces (estimated total size 4.0 MiB)
2025-04-02T18:07:57,868 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 433.9 MiB)
2025-04-02T18:07:57,869 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo [] - Added broadcast_5_piece0 in memory on 172.18.0.3:35523 (size: 34.4 KiB, free: 434.3 MiB)
2025-04-02T18:07:57,871 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.broadcast.TorrentBroadcast [] - Reading broadcast variable 5 took 9 ms
2025-04-02T18:07:57,874 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.storage.memory.MemoryStore [] - Block broadcast_5 stored as values in memory (estimated size 377.3 KiB, free 433.6 MiB)
2025-04-02T18:07:57,882 [Executor task launch worker for task 0.0 in stage 4.0 (TID 3)] INFO  org.apache.spark.executor.Executor [] - Finished task 0.0 in stage 4.0 (TID 3). 1792 bytes result sent to driver
2025-04-02T18:07:57,884 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager [] - Finished task 0.0 in stage 4.0 (TID 3) in 81 ms on 172.18.0.3 (executor 2) (1/1)
2025-04-02T18:07:57,884 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Removed TaskSet 4.0, whose tasks have all completed, from pool 
2025-04-02T18:07:57,885 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - ResultStage 4 (showString at <unknown>:0) finished in 0.092 s
2025-04-02T18:07:57,885 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
2025-04-02T18:07:57,885 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.TaskSchedulerImpl [] - Killing all running tasks in stage 4: Stage finished
2025-04-02T18:07:57,886 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler [] - Job 3 finished: showString at <unknown>:0, took 0.094530 s
2025-04-02T18:07:57,914 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator [] - Code generated in 20.612625 ms
2025-04-02T18:07:57,917 [Thread-4] INFO  org.apache.spark.SparkContext [] - SparkContext is stopping with exitCode 0.
2025-04-02T18:07:57,922 [Thread-4] INFO  org.sparkproject.jetty.server.AbstractConnector [] - Stopped Spark@2d6fb4e7{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}
2025-04-02T18:07:57,923 [Thread-4] INFO  org.apache.spark.ui.SparkUI [] - Stopped Spark web UI at http://eff501c685cd:4040
2025-04-02T18:07:57,925 [Thread-4] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend [] - Shutting down all executors
2025-04-02T18:07:57,925 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend$StandaloneDriverEndpoint [] - Asking each executor to shut down
2025-04-02T18:07:57,930 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-02T18:07:57,930 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-02T18:07:57,931 [dispatcher-Executor] INFO  org.apache.spark.executor.CoarseGrainedExecutorBackend [] - Driver commanded a shutdown
2025-04-02T18:07:57,932 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Received unregister request from application app-20250402180754-0006
2025-04-02T18:07:57,933 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.master.Master [] - Removing app app-20250402180754-0006
2025-04-02T18:07:57,938 [dispatcher-event-loop-4] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180754-0006/1
2025-04-02T18:07:57,938 [dispatcher-event-loop-8] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180754-0006/2
2025-04-02T18:07:57,939 [dispatcher-event-loop-7] INFO  org.apache.spark.deploy.worker.Worker [] - Asked to kill executor app-20250402180754-0006/0
2025-04-02T18:07:57,940 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180754-0006/1 interrupted
2025-04-02T18:07:57,940 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180754-0006/2 interrupted
2025-04-02T18:07:57,941 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Runner thread for executor app-20250402180754-0006/0 interrupted
2025-04-02T18:07:57,945 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:07:57,961 [ExecutorRunner for app-20250402180754-0006/0] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:07:57,946 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:07:57,946 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:07:57,961 [ExecutorRunner for app-20250402180754-0006/1] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:07:57,961 [ExecutorRunner for app-20250402180754-0006/2] INFO  org.apache.spark.deploy.worker.ExecutorRunner [] - Killing process!
2025-04-02T18:07:57,961 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:07:57,962 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:07:57,962 [CoarseGrainedExecutorBackend-stop-executor] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:07:57,965 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:07:57,965 [dispatcher-event-loop-2] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint [] - MapOutputTrackerMasterEndpoint stopped!
2025-04-02T18:07:57,963 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:07:57,963 [SIGTERM handler] ERROR org.apache.spark.executor.CoarseGrainedExecutorBackend [] - RECEIVED SIGNAL TERM
2025-04-02T18:07:57,966 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:07:57,969 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:07:57,970 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:07:57,975 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore [] - MemoryStore cleared
2025-04-02T18:07:57,976 [Thread-4] INFO  org.apache.spark.storage.BlockManager [] - BlockManager stopped
2025-04-02T18:07:57,977 [Thread-4] INFO  org.apache.spark.storage.BlockManagerMaster [] - BlockManagerMaster stopped
2025-04-02T18:07:57,979 [dispatcher-event-loop-2] INFO  org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint [] - OutputCommitCoordinator stopped!
2025-04-02T18:07:57,981 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - 172.18.0.2:50382 got disassociated, removing it.
2025-04-02T18:07:57,981 [dispatcher-event-loop-9] INFO  org.apache.spark.deploy.master.Master [] - eff501c685cd:45475 got disassociated, removing it.
2025-04-02T18:07:57,984 [Thread-4] INFO  org.apache.spark.SparkContext [] - Successfully stopped SparkContext
2025-04-02T18:07:57,984 [dispatcher-event-loop-0] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180754-0006/2 finished with state KILLED exitStatus 143
2025-04-02T18:07:57,984 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 2
2025-04-02T18:07:57,985 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180754-0006, execId=2)
2025-04-02T18:07:57,985 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180754-0006
2025-04-02T18:07:57,985 [dispatcher-event-loop-0] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180754-0006 removed, cleanupLocalDirs = true
2025-04-02T18:07:57,985 [dispatcher-event-loop-4] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180754-0006/2
2025-04-02T18:07:57,993 [dispatcher-event-loop-5] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180754-0006/1 finished with state KILLED exitStatus 0
2025-04-02T18:07:57,993 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 1
2025-04-02T18:07:57,993 [dispatcher-event-loop-8] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180754-0006/1
2025-04-02T18:07:57,994 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180754-0006, execId=1)
2025-04-02T18:07:57,994 [dispatcher-event-loop-1] INFO  org.apache.spark.deploy.worker.Worker [] - Executor app-20250402180754-0006/0 finished with state KILLED exitStatus 143
2025-04-02T18:07:57,994 [dispatcher-event-loop-5] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180754-0006 removed, cleanupLocalDirs = true
2025-04-02T18:07:57,994 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180754-0006
2025-04-02T18:07:57,994 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Clean up non-shuffle and non-RDD files associated with the finished executor 0
2025-04-02T18:07:57,994 [dispatcher-event-loop-2] WARN  org.apache.spark.deploy.master.Master [] - Got status update for unknown executor app-20250402180754-0006/0
2025-04-02T18:07:57,995 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Executor is not registered (appId=app-20250402180754-0006, execId=0)
2025-04-02T18:07:57,995 [dispatcher-event-loop-1] INFO  org.apache.spark.network.shuffle.ExternalShuffleBlockResolver [] - Application app-20250402180754-0006 removed, cleanupLocalDirs = true
2025-04-02T18:07:57,995 [worker-cleanup-thread] INFO  org.apache.spark.deploy.worker.Worker [] - Cleaning up local directories for application app-20250402180754-0006
2025-04-02T18:07:58,439 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Shutdown hook called
2025-04-02T18:07:58,440 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ec61dc08-1fde-410a-bfe0-5500d45ee519/pyspark-2a3b9c4b-f5cc-43c7-a201-91c86f440e9d
2025-04-02T18:07:58,443 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-ec61dc08-1fde-410a-bfe0-5500d45ee519
2025-04-02T18:07:58,445 [shutdown-hook-0] INFO  org.apache.spark.util.ShutdownHookManager [] - Deleting directory /tmp/spark-fa591a8b-24d5-4ac8-81ad-417552d69a11
